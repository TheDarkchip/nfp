{"id":"nfp-0cc","title":"Minimize module exposures after refactor","description":"Review Nfp module imports and @[expose] usage to minimize public scope while keeping proofs intact; reduce public imports where not required.","status":"closed","priority":2,"issue_type":"task","owner":"robin.gieseke@me.com","created_at":"2026-01-13T19:34:26.46573+01:00","created_by":"TheDarkchip","updated_at":"2026-01-13T20:34:08.660625+01:00","closed_at":"2026-01-13T20:34:08.660625+01:00","close_reason":"Closed","comments":[{"id":19,"issue_id":"nfp-0cc","author":"TheDarkchip","text":"Starting exposure/public-import minimization pass (focus: Model then Circuit).","created_at":"2026-01-13T18:38:59Z"},{"id":20,"issue_id":"nfp-0cc","author":"TheDarkchip","text":"Reduced exposure in Nfp/Model and Nfp/Circuit: switched to public section, then selectively re-exposed needed defs (linearRank, attention* core defs, Interface/TypedCircuit evals, InductionSpecApproxOn) and made SameInterface decidable instance private. Builds: lake build --wfail, lake build nfp --wfail.","created_at":"2026-01-13T18:49:21Z"},{"id":21,"issue_id":"nfp-0cc","author":"TheDarkchip","text":"Minimized exposures in Sound by replacing simp-on-def with public def-lemmas (using simp proofs) for ln/q/k/scores/headValue/headOutput and new dirHeadVecOfInputs_get. Added Batteries Vector lemmas import for ofFn/get, and only exposed buildInductionHeadCoreCacheWith to keep CoreSound proof definitional. Updated CoreSound/HeadOutput/LogitDiff to use *_def lemmas. lake build --wfail and lake build nfp --wfail pass.","created_at":"2026-01-13T19:20:09Z"},{"id":22,"issue_id":"nfp-0cc","author":"TheDarkchip","text":"Further reduced exposures: removed @[expose] from buildInductionHeadInputs, vRealOfInputs/valsRealOfInputs, sumFinCommonDen, dotFin; replaced rfl def-lemmas with simp proofs and updated CoreSound/HeadOutput/LogitDiff to use *_def lemmas. Remaining exposed def: buildInductionHeadCoreCacheWith (needed for definitional alignment in CoreSound). lake build --wfail and lake build nfp --wfail pass.","created_at":"2026-01-13T19:34:05Z"},{"id":23,"issue_id":"nfp-0cc","author":"TheDarkchip","text":"Removed final @[expose] in Sound by switching CoreSound/Basic to import all Nfp.Sound.Induction.Core.Basic (module-system guidance for unfolding private defs in lemma modules). buildInductionHeadCoreCacheWith is now unexposed; all Sound exposures eliminated. Builds: lake build --wfail, lake build nfp --wfail.","created_at":"2026-01-13T19:49:31Z"}]}
{"id":"nfp-0fi","title":"Add circuit-level induction scoring","description":"Extend discovery script with circuit mode combining previous-token attention head and induction stripe head; rank pairs by product of scores.","notes":"Implemented --score-mode=circuit using prev-token attention + stripe attention; outputs pair rankings and JSON; activation + embedding paths supported.","status":"closed","priority":2,"issue_type":"task","owner":"robin.gieseke@me.com","created_at":"2026-01-16T22:20:34.139359+01:00","created_by":"TheDarkchip","updated_at":"2026-01-16T22:20:37.477848+01:00","closed_at":"2026-01-16T22:20:37.477851+01:00"}
{"id":"nfp-0n4","title":"Expand circuit_copy sweep to 50 seeds","description":"Generate additional pat128 seeds and recompute circuit_copy averages to stabilize literature-parity ranking.","notes":"Completed circuit_copy sweep for seeds 2041-2075; recomputed 50-seed averages for literature-parity ranking.","status":"closed","priority":2,"issue_type":"task","assignee":"TheDarkchip","owner":"robin.gieseke@me.com","created_at":"2026-01-16T22:35:47.860141+01:00","created_by":"TheDarkchip","updated_at":"2026-01-16T22:41:03.692693+01:00","closed_at":"2026-01-16T22:41:03.692695+01:00"}
{"id":"nfp-0wx","title":"Speed up exact head-input recomputation","description":"Exact head-input recomputation in Lean is slow at nontrivial seq lengths, blocking large-scale induction-head audits. Optimize caching or algebraic reuse without weakening soundness.","acceptance_criteria":"certify_head_model inputs path is measurably faster; no soundness regressions; lake build --wfail passes.","status":"closed","priority":2,"issue_type":"task","owner":"robin.gieseke@me.com","created_at":"2026-01-14T01:52:22.383819+01:00","created_by":"TheDarkchip","updated_at":"2026-01-16T04:29:35.885165+01:00","closed_at":"2026-01-16T04:29:35.885165+01:00","close_reason":"Closed","labels":["mode:in_progress"],"comments":[{"id":31,"issue_id":"nfp-0wx","author":"TheDarkchip","text":"Model-driven certify_nonvacuous timing out. Command: lake exe nfp induction certify_nonvacuous --model models/gpt2_small_seq32.nfpt --layer 9 --head 6 --max-eps 1 --preset fast --heartbeat-ms 2000 --timing 1. Head build finishes (~287s) then stalls at 'timing: head logit-diff lower bound start' for \u003e15 min (tool timeout).","created_at":"2026-01-14T07:53:26Z"},{"id":32,"issue_id":"nfp-0wx","author":"TheDarkchip","text":"Tried optimizations: logit-diff now uses shared cache (logitDiffCache) and cacheBoundTask for epsAt/valsLo; weightBoundAt now uses cacheBound2Task. IO path uses timePureWithHeartbeat and shared cache for unweighted/weighted. Model run (certify_nonvacuous, model gpt2_small_seq32, layer 9 head 6, max-eps 1, preset fast, heartbeat 2000, timing 1) still times out at 900s. Observed timings: head build cert ~285s, unweighted logit-diff ~214s; weighted logit-diff still running at \u003e319s when timeout hit. So weighted bound remains the bottleneck.","created_at":"2026-01-14T10:40:54Z"},{"id":33,"issue_id":"nfp-0wx","author":"TheDarkchip","text":"Need exact hotspot breakdown inside weighted logit-diff; add timing/profiling (valsLo, weightBoundAt, weighted gap sum, weighted min) gated by env var (e.g., NFP_TIMING_LOGITDIFF_PROFILE). Then re-run model-driven path to identify the bottleneck; consult Lean manual on arrays/tasks/thunks/runtime for perf guidance.","created_at":"2026-01-14T11:49:54Z"},{"id":34,"issue_id":"nfp-0wx","author":"TheDarkchip","text":"Ran model-driven path with NFP_TIMING_LOGITDIFF_PROFILE=1: lake exe nfp induction certify_nonvacuous --model models/gpt2_small_seq32.nfpt --layer 9 --head 6 --max-eps 1 --preset fast --heartbeat-ms 2000 --timing 1. Profile results before timeout: valsLo force ≈307,152,836 us (~307s); weightBoundAt force still running at \u003e592,110,488 us when the 1200s timeout hit. Weighted gap sum/min never reached. Hotspot is clearly weightBoundAt force; valsLo force is secondary.","created_at":"2026-01-14T12:18:19Z"},{"id":49,"issue_id":"nfp-0wx","author":"TheDarkchip","text":"Array-backed valsLo/weightBoundAt in weighted logit-diff. Profile: valsLo ~323s, weightBoundAt ~82s, weighted gap sum/min ~2s. Non-profile certify_nonvacuous now completes before timeout (head build ~319.5s, unweighted ~224.8s, weighted ~252.6s) but ends with logitDiffLB not strictly positive (likely preexisting).","created_at":"2026-01-15T00:41:18Z"},{"id":51,"issue_id":"nfp-0wx","author":"TheDarkchip","text":"Unweighted path now array-caches epsAt/valsLo; non-profile run: head build ~325.5s, unweighted ~403.5s, weighted ~2.0s (weights already forced). Net logit-diff stage reduced to ~405s from ~476s. Still ends with logitDiffLB not strictly positive.","created_at":"2026-01-15T00:59:39Z"},{"id":53,"issue_id":"nfp-0wx","author":"TheDarkchip","text":"Array-backed wvDir (core builder) cuts runtime: head build ~92.3s, unweighted logit-diff ~100.4s, weighted ~2.0s (gpt2_small_seq32 l9 h6). Still fails with logitDiffLB not strictly positive.","created_at":"2026-01-15T01:11:41Z"},{"id":54,"issue_id":"nfp-0wx","author":"TheDarkchip","text":"Added NFP_LOGITDIFF_DEBUG in certify_nonvacuous path to print lb0/lb1 on failure. Debug run shows logitDiffLB0 already negative and logitDiffWeighted even more negative; final bound uses max=lb0. So failure is due to unweighted eps/valsLo bound being \u003c 0, not weighted gap.","created_at":"2026-01-15T01:18:55Z"},{"id":55,"issue_id":"nfp-0wx","author":"TheDarkchip","text":"Added unweighted logit-diff debug witness: new LogitDiffAtLoDebug + logitDiffLowerBoundAtLoDebug (returns proof-carrying payload) and IO prints q/prev/eps/valsPrevLo/lo/delta/gap/lbAtQ when NFP_LOGITDIFF_DEBUG and lb ≤ 0. Uses array caches + Linear.foldlFin over Fin to stay computable (avoids Finset.toList). lake build --wfail OK.","created_at":"2026-01-15T01:39:51Z"},{"id":56,"issue_id":"nfp-0wx","author":"TheDarkchip","text":"Ran NFP_LOGITDIFF_DEBUG=1 certify_nonvacuous (gpt2_small_seq32 l9 h6). Debug witness: q=31 prev=15, eps=1, valsPrevLo=50061801899493065976944889069992786787530399461209374146347821743064822202877048908162228043775421183509731469/23091082405709706454514627316409505611484660572141998578758847033440782340170124916614707389650890351739666432, lo=-8435753435854631286925054928419316119046842027277580473185950904880851966185978176390109044898313682595221599/4981499792475057450772798506692156769826437642457208145008386549235684542515587666513697100903886886354812928. valsPrevLoMinusLo=gap, so fAtQ=lbAtQ=lo (eps=1 makes bound collapse to global lo). logitDiffLB0 negative; weighted also negative; bound=max. Conclusion: failure driven by epsAt q = 1 making unweighted bound equal negative global lo.","created_at":"2026-01-15T02:12:59Z"},{"id":57,"issue_id":"nfp-0wx","author":"TheDarkchip","text":"Added per-query loAt (min of valsLo over keys excluding prev) and switched unweighted logit-diff bound to use loAt with max(0, valsPrevLo - loAt) to keep soundness when prev is global min. logitDiffLowerBoundFromCert/FromCache now call logitDiffLowerBoundAtLoAt; debug payload/output include loAt + valsPrevLoMinusLoAt. lake build --wfail passes. Next: re-run certify_nonvacuous (optionally NFP_LOGITDIFF_DEBUG=1) to see if lb0 improves vs global lo collapse.","created_at":"2026-01-15T04:50:27Z"},{"id":59,"issue_id":"nfp-0wx","author":"TheDarkchip","text":"Ran certify_nonvacuous with NFP_LOGITDIFF_DEBUG=1 after loAt change (gpt2_small_seq32 l9 h6, preset fast). Timings: head build ~92.2s; unweighted logit-diff ~106.2s; weighted ~2.0s. Debug witness still q=31 prev=15 eps=1; loAt == lo, so delta=gap and fAtQ=lb0=lo (negative). logitDiffWeighted also negative; final bound still negative.","created_at":"2026-01-15T11:02:10Z"},{"id":60,"issue_id":"nfp-0wx","author":"TheDarkchip","text":"Added debug listing for weightBoundAt=1 and loAt keys. Run (NFP_LOGITDIFF_DEBUG=1, gpt2_small_seq32 l9 h6): weightBoundAt=1 keys = {8,11,20,21,22,23,24,26,27,28,29,30,31} (valsLo listed in logs); loAt min achieved only at k=19 (weightBoundAt \u003c 1). epsAt=1 still collapses lb0 to loAt.","created_at":"2026-01-15T11:15:25Z"},{"id":61,"issue_id":"nfp-0wx","author":"TheDarkchip","text":"Tried setting fast preset splitBudgetDiffRefined=2 to trigger refineKeys. Model run (NFP_LOGITDIFF_DEBUG=1, gpt2_small_seq32 l9 h6, preset fast, heartbeat 2000, timing 1) never left head-build: heartbeat still at ~446s when 450s timeout hit. Likely refined diff makes head build far slower because weightBoundAt uses refined dotDiff for many keys. Reverted fast preset back to refined=0; refinement should be tested via explicit budget override or a more targeted path to avoid head-build regression.","created_at":"2026-01-15T11:52:25Z"},{"id":62,"issue_id":"nfp-0wx","author":"TheDarkchip","text":"Re-ran certify_nonvacuous (NFP_LOGITDIFF_DEBUG=1, gpt2_small_seq32 l9 h6, preset fast): head build ~90.2s, unweighted logit-diff ~98.2s, weighted ~2.0s. Still fails: epsAt=1 collapses lb0 to loAt=lo (min at k=19). Debug witness q=31 prev=15; logitDiffLB0 negative and bound=max remains negative.","created_at":"2026-01-15T12:07:01Z"},{"id":63,"issue_id":"nfp-0wx","author":"TheDarkchip","text":"Started refine-on-demand scaffolding: added Nfp.Sound.Induction.Refine with overlay helpers (scoreGap/weight/eps) + refine key selection; added logitDiffLowerBoundFromCacheWithEps + logitDiffLowerBoundRefinedFromCache; added buildInductionCertFromHeadWithCache? to return core cache; IO now uses cache builder and can compute refined lb in debug when NFP_LOGITDIFF_REFINE=1 (prints refine budget, key count, refined lb). No behavior change yet (debug-only).","created_at":"2026-01-15T12:40:20Z"},{"id":64,"issue_id":"nfp-0wx","author":"TheDarkchip","text":"Added refine-on-demand soundness scaffolding: new oneHot_bounds_at_of_weight_bounds, logitDiffLowerBoundFromCacheWithEps_le, and RefineSound lemmas for overlay weight bounds + refined logit-diff soundness (assuming overlay bounds + cache/cert alignment). epsAtOverlay now uses cache.cert.prev. Re-exported RefineSound. lake build --wfail OK.","created_at":"2026-01-15T13:24:08Z"},{"id":65,"issue_id":"nfp-0wx","author":"TheDarkchip","text":"Refine-on-demand soundness: replaced build-specific proof with assumption-based lemma  to avoid unfolding ; masked-case proof fixed and Build completed successfully (1906 jobs). passes. Next step is to hook this lemma to cache soundness once we expose q/k + scoreHi/scoreLoPrev bounds from core cache.","created_at":"2026-01-15T13:59:11Z"},{"id":66,"issue_id":"nfp-0wx","author":"TheDarkchip","text":"Follow-up: lemma is scoreGapLoRefinedAt_real_at_of_bounds; it avoids unfolding buildInductionHeadCoreCacheWith. Target build `lake build Nfp.Sound.Induction.RefineSound --wfail` succeeds.","created_at":"2026-01-15T13:59:26Z"},{"id":67,"issue_id":"nfp-0wx","author":"TheDarkchip","text":"Added core cache bound lemmas in CoreSound/Basic: buildInductionHeadCoreCacheWith_bounds plus q_bounds/k_bounds/score_bounds corollaries; split long line for lint. lake build Nfp.Sound.Induction.CoreSound.Basic --wfail timed out after 10m.","created_at":"2026-01-15T14:37:31Z"},{"id":76,"issue_id":"nfp-0wx","author":"TheDarkchip","text":"Tried to eliminate CertSound heartbeats via refactors: moved logit-bound → ValueIntervalBounds into new lemma  in  (now imports ), added  lemma in . Updated  to take  + definitional equalities; CertSound now uses . Reworked cert as structure literal to make  definally. Still hitting elaborator/defeq heartbeats in  (record construction for  + cache cert defeq). Using  simplified proofs but broke ; reverted to array-backed  + simp-only for  equality;  rfl holds again. Current blocker: defeq/whnf timeouts when building soundness record (line ~1261). Likely needs structural refactor to avoid defeq on / fields (e.g., lemma that packages  for explicit record literal, or move cert-field alignment into a separate lemma to avoid elaborator reduction).","created_at":"2026-01-15T22:37:34Z"},{"id":77,"issue_id":"nfp-0wx","author":"TheDarkchip","text":"Follow-up (previous comment had shell quoting issues): moved logit-bound to ValueIntervalBounds into a new lemma valCert_bounds_of_ln_bounds in Nfp/Sound/Induction/CoreSound/Values.lean (Values now imports Nfp.Sound.Induction.Core.Basic). Added buildInductionHeadCert_values lemma in Nfp/Sound/Induction/Core/Basic.lean. Updated valsReal_bounds_at_of_ln_bounds to accept valsLo/valsHi + definitional equalities; CertSound now uses valCert_bounds_of_ln_bounds. cert now built as a structure literal to make values := valCert definitional. Reverted to array-backed wvDir (simp-only for wvDir equality) so hcert_eq rfl still holds; using wvDirTask breaks hcert_eq. Still hitting elaborator/defeq heartbeats when constructing InductionHeadCertSound record in CertSound (line ~1261). Likely need structural refactor to avoid defeq on certFields/cert fields (e.g., new lemma that packages InductionHeadCertSound for explicit record literal or move cert-field alignment into a lemma to avoid elaborator reduction).","created_at":"2026-01-15T22:37:46Z"},{"id":80,"issue_id":"nfp-0wx","author":"TheDarkchip","text":"Implemented refine-on-demand default for unweighted logit-diff: added logitDiffLowerBoundRefineOnDemand(+WithSpec) in LogitDiff, with soundness lemmas in RefineSound; IO now uses refine-on-demand for lb0. Added logitDiffCache_def + logitDiffLowerBoundFromCache_eq_withEps for unfolding. Builds pass: lake build --wfail and lake build nfp --wfail.","created_at":"2026-01-16T02:24:43Z"},{"id":81,"issue_id":"nfp-0wx","author":"TheDarkchip","text":"Ran NFP_LOGITDIFF_DEBUG=1 after refine-on-demand default (gpt2_small_seq32 l9 h6, preset fast). head build ~86.2s; unweighted logit-diff ~146.3s. Debug witness still q=31 prev=15, epsAt=1, loAt=lo. logitDiffLB0 unchanged and negative; refine-on-demand did not lift epsAt (weightSum still huge), weighted bound also negative. Likely need refine spec that targets keys with weightBoundAt=1 (13 keys in debug) or otherwise reduces epsAt below 1; current refineKeysAt (negative gaps + worst key) is insufficient.","created_at":"2026-01-16T02:33:21Z"},{"id":82,"issue_id":"nfp-0wx","author":"TheDarkchip","text":"Implemented argmin-targeted refine-on-demand: added logitDiffLowerBoundArgminFromCache, refineSpecForQueryWithWeightOnes, updated logitDiffLowerBoundRefineOnDemand + soundness + IO debug. Builds: lake build --wfail and lake build nfp --wfail OK.\n\nRan: NFP_LOGITDIFF_DEBUG=1 lake exe nfp induction certify_nonvacuous --model models/gpt2_small_seq32.nfpt --layer 9 --head 6 --max-eps 1 --preset fast --heartbeat-ms 2000 --timing 1. Timings: head build ~82.3s; unweighted logit-diff ~235.0s; weighted ~2.0s. Debug witness still q=31 prev=15 with eps=1 and loAt=lo (min at k=19); logitDiffLB0 negative and unchanged; logitDiffWeighted more negative; final bound uses max=lb0. WeightBoundAt=1 keys list unchanged (13 keys). So argmin+weight-one refinement did not lift epsAt/loAt enough; bound still fails strictly positive.\n\nLikely next: refine valsLo for the loAt-min key(s) (k=19) or add targeted refinement to reduce epsAt below 1 for the weight=1 keys; consider logging refined lb with NFP_LOGITDIFF_REFINE=1 to confirm no improvement.\n","created_at":"2026-01-16T03:19:38Z"}]}
{"id":"nfp-0wx.1","title":"State change: mode → in_progress","description":"Set mode to in_progress\n\nReason: Active optimization work on logit-diff weighted bound","status":"closed","priority":4,"issue_type":"event","created_at":"2026-01-14T11:42:02.855327+01:00","created_by":"TheDarkchip","updated_at":"2026-01-14T11:42:02.855327+01:00","closed_at":"2026-01-14T11:42:03.855327+01:00","dependencies":[{"issue_id":"nfp-0wx.1","depends_on_id":"nfp-0wx","type":"parent-child","created_at":"2026-01-14T11:42:02.856021+01:00","created_by":"TheDarkchip"}]}
{"id":"nfp-0wx.2","title":"Optimize weightBoundAt force hotspot","description":"Profiling with NFP_TIMING_LOGITDIFF_PROFILE shows weightBoundAt force dominating weighted logit-diff. Investigate caching/array layout, task scheduling, and redundant loops to reduce wall time.","acceptance_criteria":"weightBoundAt force no longer dominates weighted logit-diff on gpt2_small_seq32 l9 h6; certify_nonvacuous completes without timeout","status":"closed","priority":2,"issue_type":"task","owner":"robin.gieseke@me.com","created_at":"2026-01-14T13:21:27.888545+01:00","created_by":"TheDarkchip","updated_at":"2026-01-16T03:07:55.515402+01:00","closed_at":"2026-01-16T03:07:55.515402+01:00","close_reason":"Closed","dependencies":[{"issue_id":"nfp-0wx.2","depends_on_id":"nfp-0wx","type":"parent-child","created_at":"2026-01-14T13:21:27.891048+01:00","created_by":"TheDarkchip"}],"comments":[{"id":35,"issue_id":"nfp-0wx.2","author":"TheDarkchip","text":"Added cached weightBoundAtBase (Bounds.cacheBound2) reused by epsAt/weightBoundAt; updated CoreSound proofs to rewrite via cacheBound2_apply. lake build --wfail passes.","created_at":"2026-01-14T12:41:18Z"},{"id":36,"issue_id":"nfp-0wx.2","author":"TheDarkchip","text":"Profile run after weightBoundAtBase caching (before qLo/kLo cache): head build ~349s, valsLo force ~322s, weightBoundAt force still running at ~500s when 1200s timeout hit; weighted gap/min not reached. Tried caching qLo/qHi/kLo/kHi via cacheBound2; head build induction cert no longer returned within 1200s (stuck at ~1170s), so reverted.","created_at":"2026-01-14T13:33:30Z"},{"id":37,"issue_id":"nfp-0wx.2","author":"TheDarkchip","text":"Ran certify_nonvacuous without logit-diff profiling after reverting qLo/kLo cache: head build cert ~347.6s, unweighted logit-diff ~280.6s; weighted logit-diff still running at ~544s when 1200s timeout hit (total \u003e1170s). Weighted stage remains bottleneck.","created_at":"2026-01-14T13:59:57Z"},{"id":38,"issue_id":"nfp-0wx.2","author":"TheDarkchip","text":"Added conditional in logitDiffLowerBoundWeightedAt to skip weightBoundAt when max(0,diff)=0; build passes. certify_nonvacuous timing: head build ~349s, unweighted ~250.5s; weighted still running at ~571s when 1200s timeout hit (no material improvement).","created_at":"2026-01-14T14:27:51Z"},{"id":39,"issue_id":"nfp-0wx.2","author":"TheDarkchip","text":"Added nonvacuous path precompute for weightBoundAt: spawn per-active-q tasks to force a single k per row (fills cache), overlapping with unweighted logit-diff; wait on precompute only when weighted needed; skip precompute when NFP_TIMING_LOGITDIFF_PROFILE is set; cancel task when weighted not needed. See Nfp/IO/InductionHead/Basic.lean.","created_at":"2026-01-14T15:24:56Z"},{"id":40,"issue_id":"nfp-0wx.2","author":"TheDarkchip","text":"Reworked nonvacuous path to spawn weighted logit-diff in background (skip when profiling), then await task if needed. Removed weightBoundAt precompute. Build nfp --wfail ok. Model run still timed out at 1200s: head build ~347.6s, unweighted ~283.1s; weighted stage still running \u003e564s at timeout. No material improvement yet.","created_at":"2026-01-14T16:14:44Z"},{"id":41,"issue_id":"nfp-0wx.2","author":"TheDarkchip","text":"Profile (NFP_TIMING_LOGITDIFF_PROFILE=1) after background weighted task change: valsLo force ≈285s; weightBoundAt force still running \u003e564s when 1200s timeout hit. Weighted gap sum/min not reached. Hotspot remains weightBoundAt force.","created_at":"2026-01-14T16:43:49Z"},{"id":42,"issue_id":"nfp-0wx.2","author":"TheDarkchip","text":"Added a no-refine fast path: when splitBudgetDiffRefined = splitBudgetDiffBase, dotDiffLo/Hi use base bounds directly and worstKey is disabled (returns none), avoiding duplicate scoreGapLoBase work. Updated CoreSound proof to branch on the budget equality. lake build --wfail passes.","created_at":"2026-01-14T17:34:47Z"},{"id":43,"issue_id":"nfp-0wx.2","author":"TheDarkchip","text":"Removed redundant weightBoundAt clamping (weightBoundAt now uses base cached bounds directly; epsAt still computed separately). Added an unfolding lemma for logitDiffLowerBoundWeightedAt and switched logitDiffLowerBoundWeightedFromCache to compute per-q weighted gaps via Bounds.cacheBoundTask (parallel per query). Profiling run (NFP_TIMING_LOGITDIFF_PROFILE=1, gpt2_small_seq32 l9 h6, preset fast): head build ≈322s, valsLo force ≈281s, weightBoundAt force ≈80s (down from \u003e500s), weighted gap sum still running at ≈487s when 1200s timeout hit. Non-profile run: head build ≈313s, unweighted logit-diff ≈150s, weighted stage still running at ≈708s when 1200s timeout hit. Weighted gap sum remains bottleneck.","created_at":"2026-01-14T18:58:54Z"},{"id":45,"issue_id":"nfp-0wx.2","author":"TheDarkchip","text":"Switched weighted gap sum to tail-recursive Linear.sumFin (see Nfp/Sound/Induction/LogitDiff.lean + Nfp/IO/Timing.lean). lake build --wfail ok. Profile run (NFP_TIMING_LOGITDIFF_PROFILE=1, gpt2_small_seq32 l9 h6): head build ~323.5s; valsLo force ~325.3s; weightBoundAt force ~82.3s; weighted gap sum still running at ~459.9s when 1200s timeout hit. Weighted gap sum remains bottleneck; weightBoundAt no longer dominant.","created_at":"2026-01-14T23:34:52Z"},{"id":48,"issue_id":"nfp-0wx.2","author":"TheDarkchip","text":"With array-backed weighted gap: profile (NFP_TIMING_LOGITDIFF_PROFILE=1, gpt2_small_seq32 l9 h6) valsLo force ~323.2s, weightBoundAt force ~82.4s. Non-profile run now completes before timeout: head build ~319.5s, unweighted logit-diff ~224.8s, weighted logit-diff ~252.6s. Run ends with logitDiffLB not strictly positive (likely preexisting), but no timeout.","created_at":"2026-01-15T00:41:12Z"},{"id":50,"issue_id":"nfp-0wx.2","author":"TheDarkchip","text":"Array-cached epsAt/valsLo in logitDiffLowerBoundFromCache. Non-profile run (gpt2_small_seq32 l9 h6): head build ~325.5s; unweighted logit-diff ~403.5s (higher due to forcing epsAt for all q); weighted logit-diff now ~2.0s (likely weightBoundAt already forced). Net logit-diff stage ~405s vs ~476s prior. Still ends with logitDiffLB not strictly positive.","created_at":"2026-01-15T00:59:35Z"},{"id":52,"issue_id":"nfp-0wx.2","author":"TheDarkchip","text":"After array-backed wvDir in core builder: non-profile certify_nonvacuous (gpt2_small_seq32 l9 h6) now much faster: head build ~92.3s, unweighted logit-diff ~100.4s, weighted ~2.0s; total logit-diff stage ~102s. Still ends with logitDiffLB not strictly positive.","created_at":"2026-01-15T01:11:37Z"},{"id":58,"issue_id":"nfp-0wx.2","author":"TheDarkchip","text":"No new weightBoundAt optimizations in this round; focused on improving unweighted logit-diff bound (per-query loAt) to avoid epsAt=1 collapsing to global lo. This may reduce weighted-bound usage, but weightBoundAt hotspot work is unchanged.","created_at":"2026-01-15T04:50:33Z"},{"id":68,"issue_id":"nfp-0wx.2","author":"TheDarkchip","text":"Added `Nfp.Tactic.Linter.NoHeartbeats` (and `Nfp.Tactic.Linter` aggregator), enabled `weak.linter.nfp.noHeartbeats`, and imported the linter from `Nfp.Core.Basic` + `Nfp.System.Dag` so it is loaded broadly. Removed `set_option maxHeartbeats`/`synthInstance.maxHeartbeats` from `Nfp/Sound/Induction/CoreSound/Basic.lean`. Built `lake build Nfp.Tactic.Linter.NoHeartbeats --wfail`.\n","created_at":"2026-01-15T16:05:47Z"},{"id":69,"issue_id":"nfp-0wx.2","author":"TheDarkchip","text":"Progress: CacheBounds now compiles after refactoring dotLo/dotHi simplification and consolidating the final cache-field rewrite to a single simpa. Build completed successfully (1902 jobs). succeeds. CertSound still hits deterministic heartbeats around cert/cache.cert equality; tried rfl + field-wise simp (zeta := false) to avoid defeq, but Lean still times out while unfolding cache fields. Next likely step: refactor to share a cert builder between Core.Basic + CertSound (avoid definitional equality) or restructure the success branch to use cache fields directly.","created_at":"2026-01-15T17:15:58Z"},{"id":70,"issue_id":"nfp-0wx.2","author":"TheDarkchip","text":"Refactor: added InductionHeadCertFields + buildInductionHeadCertFields (with def lemma) in Core.Basic; buildInductionHeadCoreCacheWith and CertSound now use certFields for margin/eps/weightBoundAt; updated simp sites. Core.Basic builds clean. CertSound still hits deterministic heartbeats at hcert_eq (simpa [cert] using buildInductionHeadCoreCacheWith_cert_eq). Need further refactor: either move cert equality to a lemma in Core.Basic to avoid defeq or restructure CertSound to avoid cache.cert defeq.","created_at":"2026-01-15T17:59:16Z"},{"id":79,"issue_id":"nfp-0wx.2","author":"TheDarkchip","text":"Profile run (NFP_TIMING_LOGITDIFF_PROFILE=1, gpt2_small_seq32 l9 h6, preset fast): head build ~92.2s; valsLo force ~2.0s; weightBoundAt force ~76.1s; weighted gap sum ~2.0s; weighted min ~2.0s. Unweighted logit-diff ~108.2s; weighted ~76.1s. weightBoundAt no longer dominates. Run terminates with negative logitDiffLB (same as before).","created_at":"2026-01-16T02:06:25Z"}]}
{"id":"nfp-0wx.3","title":"Optimize weighted logit-diff gap sum","description":"Profile runs show weighted logit-diff now dominated by per-query gap sum (sum of weightBoundAt * max(0,diff)) even after weightBoundAt cleanup. Investigate faster accumulation (arrays/fin fold), concurrency across queries, or bounded-precision Rat strategies without weakening soundness.","acceptance_criteria":"On gpt2_small_seq32 l9 h6 preset fast, weighted logit-diff stage completes without 1200s timeout; timing output shows weighted stage significantly below ~700s.","status":"closed","priority":2,"issue_type":"task","owner":"robin.gieseke@me.com","created_at":"2026-01-14T19:59:48.477668+01:00","created_by":"TheDarkchip","updated_at":"2026-01-16T04:29:31.803995+01:00","closed_at":"2026-01-16T04:29:31.803995+01:00","close_reason":"Closed","dependencies":[{"issue_id":"nfp-0wx.3","depends_on_id":"nfp-0wx","type":"parent-child","created_at":"2026-01-14T19:59:48.481638+01:00","created_by":"TheDarkchip"}],"comments":[{"id":44,"issue_id":"nfp-0wx.3","author":"TheDarkchip","text":"Tried structural simplifications: logitDiffLowerBound* now use active.inf' (no image/min), weighted gap sum uses Finset.univ (no erase) and drops diffPos=0 branch; precompute valsLoPrev; profiling updated. Builds pass. Profiling (NFP_TIMING_LOGITDIFF_PROFILE=1) still times out: head build ~323s, valsLo force ~305s, weightBoundAt force ~82s, weighted gap sum still running at ~484s when 1200s timeout hit. Non-profile run still timed out: head build ~323s, unweighted ~180.7s, weighted stage still running at ~692s when timeout hit. Bottleneck remains weighted gap sum.","created_at":"2026-01-14T22:29:07Z"},{"id":46,"issue_id":"nfp-0wx.3","author":"TheDarkchip","text":"Tried tail-recursive Linear.sumFin for weighted gap sum (LogitDiff + Timing). Profile (NFP_TIMING_LOGITDIFF_PROFILE=1, gpt2_small_seq32 l9 h6): head build ~323.5s; valsLo force ~325.3s; weightBoundAt force ~82.3s; weighted gap sum still running at ~459.9s when 1200s timeout hit. Minor improvement vs ~487s previously; still main bottleneck.","created_at":"2026-01-14T23:34:58Z"},{"id":47,"issue_id":"nfp-0wx.3","author":"TheDarkchip","text":"Implemented array-backed valsLo/weightBoundAt in logitDiffLowerBoundWeightedFromCache. Profile (NFP_TIMING_LOGITDIFF_PROFILE=1, gpt2_small_seq32 l9 h6): valsLo force ~323.2s; weightBoundAt force ~82.4s; weighted gap sum ~2.0s; weighted min ~2.0s. Weighted gap sum no longer bottleneck.","created_at":"2026-01-15T00:41:06Z"}]}
{"id":"nfp-0wx.4","title":"Refactor CertSound to avoid cache cert defeq heartbeats","description":"CertSound currently redefines the core cache lets; proving cache.cert = local cert triggers definitional-equality heartbeats. Refactor to reuse cache fields or factor shared builders to avoid defeq-heavy proofs.","acceptance_criteria":"CertSound compiles without defeq timeouts; no soundness regressions; lake build --wfail passes.","status":"closed","priority":2,"issue_type":"task","owner":"robin.gieseke@me.com","created_at":"2026-01-15T19:14:20.80418+01:00","created_by":"TheDarkchip","updated_at":"2026-01-16T03:08:01.599555+01:00","closed_at":"2026-01-16T03:08:01.599555+01:00","close_reason":"Closed","dependencies":[{"issue_id":"nfp-0wx.4","depends_on_id":"nfp-0wx","type":"parent-child","created_at":"2026-01-15T19:14:20.808146+01:00","created_by":"TheDarkchip"}],"comments":[{"id":71,"issue_id":"nfp-0wx.4","author":"TheDarkchip","text":"Tried refactor in CertSound to avoid cache.cert defeq. Switched cert to local buildInductionHeadCert inputs certFields valCert and attempted to show cache.cert = cert (rfl/simp), but simp on buildInductionHeadCoreCacheWith is still too heavy; now heartbeats/timeouts appear in hsoftmax_bounds (line ~1193) and hvals_bounds (line ~1236) even after trimming simp. Current build: lake build Nfp.Sound.Induction.CoreSound.Basic.CertSound --wfail fails with defeq/heartbeat timeouts (first at line ~1193). Suggest next: either revert to cache cert and rewrite proofs to use cache fields directly, or add small Core.Basic lemmas that expose cache field equalities to avoid unfolding buildInductionHeadCoreCacheWith; then use those to rewrite without defeq.","created_at":"2026-01-15T19:44:44Z"},{"id":72,"issue_id":"nfp-0wx.4","author":"TheDarkchip","text":"Option 1 in progress: added Core.Basic lemma buildInductionHeadValCert_bounds_at (plus valsLo/valsHi projections) and refactored CertSound hvals_bounds_at to use it. Core.Basic now builds. CertSound still hits deterministic isDefEq heartbeat around line ~1252 in hvals_bounds_at (valsLo/valsHi real coercion), even after removing simp; last run shows isDefEq timeout at hvalsLo_real. Earlier diagnostics with set_option diagnostics true showed simp thrash in other spots, but now bottleneck is defeq on valsLo/valsHi real coercions. Likely need to avoid coercion equality in CertSound or move the valsLo/valsHi Real-coercion rewrite into Core.Basic/Bounds lemma.","created_at":"2026-01-15T20:10:02Z"},{"id":73,"issue_id":"nfp-0wx.4","author":"TheDarkchip","text":"Tried to tame CertSound heartbeats: added valsReal alias (valsRealOfInputs inputs), switched simp to simp only, refactored hvals_bounds_at with lnLo_k/lnHi_k/lnReal_k and dotLower/dotUpper/dotReal lets, and made dotIntervalLower/Upper inequalities explicit. Build still fails with deterministic heartbeats around hlow' elaboration (line ~1244) in hvals_bounds_at; now error is elaborator/isDefEq at the dotIntervalLower_le_dotProduct_real_add application. set_option diagnostics true at top-level triggers private constant error (_private...buildInductionHeadCoreCacheWith.match_1). Next likely step: avoid array-backed wvDir in proof (rewrite to wvDirTask) or extract the dotIntervalLower/dotProduct bounds into a standalone lemma to reduce elaboration context, possibly with local diagnostics in that lemma.","created_at":"2026-01-15T20:40:04Z"},{"id":74,"issue_id":"nfp-0wx.4","author":"TheDarkchip","text":"Tried replacing simpa with explicit rw/exact and simplifying hlow''/hhigh'' (removed add_le_add_right chain, inlined dotIntervalLower_le_dotProduct_real_add). Still hits deterministic heartbeats at the rewrite of valsLo (line ~1244):  in hlow'' times out; simp only also times out. Suggest next step: avoid rewriting valsLo in CertSound by proving a lemma that relates valsLo to the dotIntervalLower expression without unfolding array-backed wvDir (e.g. use wvDirTask or a small ext lemma), or restructure to use core cache cert directly.","created_at":"2026-01-15T20:55:39Z"},{"id":75,"issue_id":"nfp-0wx.4","author":"TheDarkchip","text":"Note: rw [valsLo, Rat.cast_add, add_comm] in hlow still times out (zsh globbing ate the snippet in prior comment). The hotspot remains the rewrite of valsLo; simp only also times out.","created_at":"2026-01-15T20:56:12Z"},{"id":78,"issue_id":"nfp-0wx.4","author":"TheDarkchip","text":"Refactor success: added Core.Basic lemma buildInductionHeadCertFields_eps_eq and rewrote CertSound hepsAt_le_eps to use it (avoids unfolding buildInductionHeadCertFields). cert now uses local eps/epsAt/weightBoundAt/margin, plus field-wise rewrite, and long-line lints fixed. set_option diagnostics true used temporarily for simp hotspots, then removed. Target build passes: lake build Nfp.Sound.Induction.CoreSound.Basic.CertSound --wfail.","created_at":"2026-01-16T01:51:14Z"}]}
{"id":"nfp-0z1","title":"Stop building SoftmaxMargin/ValueRange certs in trusted Sound; require explicit cert inputs","description":"Summary\n- The CLI currently accepts raw scores/weights/values and calls trusted builders to synthesize certificates inside Lean.\n\nViolation\n- Intended split requires untrusted generation and trusted verification. `buildSoftmaxMarginCert?` and `buildValueRangeCert?` compute certificates from raw data inside trusted `Nfp.Sound.*`.\n\nEvidence\n- `Nfp/Sound/Induction/Core/Basic.lean`: `buildSoftmaxMarginCert?`, `buildValueRangeCert?`.\n- `Nfp/IO/Run/Basic.lean`: `runInductionCertify*` calls these builders after parsing raw inputs.\n\nImpact\n- Trusted verification is no longer a small checker; it recomputes certs from raw data.\n- Makes external cert generation (e.g., Python or other tooling) unusable because CLI ignores explicit cert payloads.\n\nProposed Fix\n- Accept explicit `SoftmaxMarginCert` and `ValueRangeCert` inputs (use `Nfp/IO/Pure/SoftmaxMargin/Cert.lean` and `Nfp/IO/Pure/ValueRange/Cert.lean` parsers).\n- Verify using `Circuit.checkSoftmaxMarginCert` / `Circuit.checkValueRangeCert`.\n- Move builder functions to untrusted tooling (or keep them in untrusted modules only).\n\nDependencies / Blockers\n- None strictly, but ideally coordinate with the broader trust-boundary cleanup for IO.Pure parsers.\n","notes":"Removed buildSoftmaxMarginCert?/buildValueRangeCert? and deleted IO.runInductionCertify* paths; CLI now accepts explicit induction-head certs only.","status":"closed","priority":2,"issue_type":"task","owner":"robin.gieseke@me.com","created_at":"2026-01-17T07:00:31.432183+01:00","created_by":"TheDarkchip","updated_at":"2026-01-17T07:51:49.919248+01:00","closed_at":"2026-01-17T07:51:49.91925+01:00"}
{"id":"nfp-140","title":"Use real layer activations for induction discovery scoring","description":"Discovery currently computes Q/K/V from LN(embeddings) in scripts/discover_gpt2_induction_targets.py, which ignores prior-layer residual streams. Canonical induction heads rely on earlier heads (previous-token head) and only appear on true layer activations, so embedding-only scans can miss standard heads. Add a discovery mode that runs a forward pass (transformers) to extract per-layer residual stream activations, then computes Q/K/V (with ln_1) for each head and scores prefix-matching/copying on those activations.\\n\\nAcceptance criteria:\\n- new discovery mode (e.g., --use-activations) uses real layer residuals for Q/K/V.\\n- results emitted in TXT/JSON with clear metadata.\\n- existing embedding-only path remains available.\\n\\nDepends on:\\n- nfp-lok (attention-based discovery pipeline).","notes":"Added --use-activations/--hf-model/--device for discovery; activation path computes QKV/attn from HF model and skips NFP weights; report+JSON include activation metadata. py_compile failed due to permissions writing cache.","status":"closed","priority":2,"issue_type":"task","assignee":"TheDarkchip","owner":"robin.gieseke@me.com","created_at":"2026-01-16T21:31:57.302506+01:00","created_by":"TheDarkchip","updated_at":"2026-01-16T21:42:30.518881+01:00","closed_at":"2026-01-16T21:42:30.518885+01:00"}
{"id":"nfp-1gd","title":"Align claims and soundness limitations with induction certification audit","description":"Update CLAIMS.md and SOUNDNESS_LIMITATIONS.md to reflect conditional head-level certification and updated CLI names.","status":"closed","priority":2,"issue_type":"task","owner":"robin.gieseke@me.com","created_at":"2026-01-13T08:13:22.727618+01:00","created_by":"TheDarkchip","updated_at":"2026-01-13T08:13:41.983105+01:00","closed_at":"2026-01-13T08:13:41.983105+01:00","close_reason":"Closed"}
{"id":"nfp-1hc","title":"Add untrusted direction search for non-vacuous certs","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-17T10:22:48.673289+01:00","updated_at":"2026-01-17T10:22:48.739537+01:00","closed_at":"2026-01-17T10:22:48.739537+01:00","close_reason":"Added search-direction flags to generator and documented non-vacuous workflow."}
{"id":"nfp-1mx","title":"Investigate bounded-precision Rat rounding for induction cert speed","description":"Implement non-identity ratRoundDown/ratRoundUp (or local rounding) to control denominator growth while preserving bounds; measure impact on induction-head certification runtime.","status":"closed","priority":2,"issue_type":"task","owner":"robin.gieseke@me.com","created_at":"2026-01-13T04:06:14.596939+01:00","created_by":"TheDarkchip","updated_at":"2026-01-16T18:14:28.138119+01:00","closed_at":"2026-01-16T18:14:28.138119+01:00","close_reason":"Out of scope for literature parity; decision already recorded to keep full-precision Rat bounds.","comments":[{"id":3,"issue_id":"nfp-1mx","author":"TheDarkchip","text":"Decision: keep full-precision Rat bounds for now; no bounded-precision rounding changes planned until further evidence.","created_at":"2026-01-13T06:51:29Z"}]}
{"id":"nfp-1zd","title":"Investigate negative logitDiffLB in certify_nonvacuous","description":"certify_nonvacuous now runs in time but logitDiffLB stays negative because epsAt=1 collapses lb0 to loAt (min at k=19) and refine-on-demand (argmin + weight=1 keys) does not lift epsAt/loAt. Need a targeted refinement strategy for loAt-min keys or weight=1 keys that reduces epsAt below 1 without exploding head-build time.","acceptance_criteria":"On gpt2_small_seq32 l9 h6 preset fast, NFP_LOGITDIFF_DEBUG shows lb0 \u003e= 0 (or positive) with epsAt \u003c 1 for the argmin query; certify_nonvacuous passes the logitDiffLB check.","status":"open","priority":2,"issue_type":"task","owner":"robin.gieseke@me.com","created_at":"2026-01-16T04:29:26.182389+01:00","created_by":"TheDarkchip","updated_at":"2026-01-16T04:29:26.182389+01:00","labels":["mode:in_progress"],"dependencies":[{"issue_id":"nfp-1zd","depends_on_id":"nfp-0wx","type":"discovered-from","created_at":"2026-01-16T04:29:26.189595+01:00","created_by":"TheDarkchip"}],"comments":[{"id":83,"issue_id":"nfp-1zd","author":"TheDarkchip","text":"Added refineBudgetBoost (max(b+1, 2*b)) and second-pass refinement for argmin + weight-one keys when lb0≤0 and lb01≤0. Build passes. Re-ran: NFP_LOGITDIFF_DEBUG=1 lake exe nfp induction certify_nonvacuous --model models/gpt2_small_seq32.nfpt --layer 9 --head 6 --max-eps 1 --preset fast --heartbeat-ms 2000 --timing 1. Timings: head build ~88.3s; unweighted logit-diff ~340.8s; weighted ~2.0s. Debug witness unchanged (q=31 prev=15 eps=1 loAt=lo), final logitDiffLB still negative (same as lb0).","created_at":"2026-01-16T04:05:14Z"},{"id":84,"issue_id":"nfp-1zd","author":"TheDarkchip","text":"Refactored logit-diff soundness: moved unweighted soundness lemmas into new Nfp/Sound/Induction/LogitDiffSound.lean, added headLogitDiff_def, valsLoRefinedAt_def, sqrtLowerScale_def. Updated RefineSound to use *_def lemmas and cleaned algebra; fixed long-line lint. lake build --wfail passes. Next: rerun certify_nonvacuous (NFP_LOGITDIFF_DEBUG=1) to see if value-refinement improves lb0 (epsAt\u003c1).","created_at":"2026-01-16T05:08:16Z"},{"id":85,"issue_id":"nfp-1zd","author":"TheDarkchip","text":"Ran NFP_LOGITDIFF_DEBUG=1 certify_nonvacuous (gpt2_small_seq32 l9 h6, preset fast) with 15m timeout; build/compile completed, head build cert ~90s, then unweighted logit-diff kept running past 775s when command timed out at 900s. No lb0/epsAt output yet. Consider rerunning with longer timeout or adding a shorter debug path (e.g., early-exit after first q) to capture epsAt/loAt.","created_at":"2026-01-16T11:57:26Z"},{"id":86,"issue_id":"nfp-1zd","author":"TheDarkchip","text":"Split IO induction-head nonvacuous paths into Nfp/IO/InductionHead/Nonvacuous.lean to satisfy long-file lint; made shared helpers (configureTiming, splitConfigFromOptions, ratToString/ratOptToString, logitDiffDebug*, deriveDirectionFromTokens) public so Nonvacuous can reuse. lake build --wfail and lake build nfp --wfail OK; lake exe nfp --help OK.","created_at":"2026-01-16T12:31:46Z"},{"id":87,"issue_id":"nfp-1zd","author":"TheDarkchip","text":"Generalized refine-on-demand keys: refineKeysAtWithWeightOnes now unions loAt-min keys (loAtKeysAt) in addition to weight-one + negative-gap/worst key. Updated docstrings accordingly. Builds: lake build Nfp.Sound.Induction.Refine --wfail and lake build --wfail OK.","created_at":"2026-01-16T12:53:43Z"},{"id":88,"issue_id":"nfp-1zd","author":"TheDarkchip","text":"Added top-weight refinement: new refineTopWeightCount (min 8 (max 1 (2*budget))) + topWeightKeysAt (Array.qsort by weight) and included in refineKeysAtWithWeightOnes (now takes budget). Updated IO debug to pass refineBudget. Builds: lake build Nfp.Sound.Induction.Refine --wfail, lake build --wfail. Early-exit run still shows epsAt=1, loAt=lo, same q=31/prev=15; weightBoundAt=1 keys unchanged; loAt key still k=19.","created_at":"2026-01-16T13:15:17Z"},{"id":93,"issue_id":"nfp-1zd","author":"TheDarkchip","text":"Alt-bound diag (new NFP_LOGITDIFF_ALT_BOUND_Q=31) using scoreLoPrev-scoreHi with exp lower bound (piecewise linear/quadratic) shows no improvement: g\u003e=0 count=18, g\u003e-1 count=26, minGap≈-2.7407, so at least 5 keys have g\u003c=-1. The summed epsAt_alt is \u003e5 (printed huge Rat). Conclusion: cheap exp-lower-bound softmax weighting from scoreLo/scoreHi cannot drop epsAt below 1 on q=31.","created_at":"2026-01-16T14:50:06Z"},{"id":94,"issue_id":"nfp-1zd","author":"TheDarkchip","text":"Added q-only logit-diff diagnostics to speed evaluation: env NFP_LOGITDIFF_Q_ONLY=\u003cq\u003e prints epsAt for that q; optional NFP_LOGITDIFF_Q_ONLY_VALS=1 adds loAt/valsPrevLo/lbAtQ; NFP_LOGITDIFF_Q_ONLY_REFINE=1 prints refined epsAt (and refined lb if VALS=1); NFP_LOGITDIFF_Q_ONLY_EARLY_EXIT=1 exits before full logit-diff.","created_at":"2026-01-16T15:02:52Z"},{"id":95,"issue_id":"nfp-1zd","author":"TheDarkchip","text":"Python sweep (exact scores) for L9H6 across models: gpt2_small_seq32 eps_true min=0.9516 max=0.9987; gpt2_rigorous_seq32 min=0.9291 max=0.9971; gpt2_small (seq256) min=0.9834 max=0.99997. No active queries with eps\u003c0.90; thus any score-only 'better math' cannot lower epsAt below ~0.93–0.98 for this head/prompt.","created_at":"2026-01-16T15:14:43Z"},{"id":96,"issue_id":"nfp-1zd","author":"TheDarkchip","text":"Head sweep gpt2_rigorous_seq32 (exact eps over active queries): best mins: L1H9 min=0.0027 (med≈0.99999), L1H8 min=0.117, L0H5 min=0.209 (med≈0.379, max≈0.446), L10H11 min=0.329 (med≈0.893). Worst heads still near 1. This suggests L0H5 is the most promising for 'better math' because med/max eps are materially lower.","created_at":"2026-01-16T15:20:59Z"},{"id":109,"issue_id":"nfp-1zd","author":"TheDarkchip","text":"With period=16 on gpt2_rigorous_seq32.nfpt, certify still reports epsAt=1 for L1H6 (even though untrusted stripe scan estimates eps≈0.446). max-eps 1/2 fails immediately; q-only logit-diff debug confirms epsAt=1 and nonvacuous run times out. Suggests epsAt/loAt bounds still too loose for induction head despite correct period.","created_at":"2026-01-17T00:49:04Z"},{"id":111,"issue_id":"nfp-1zd","author":"TheDarkchip","text":"Added low-value key refinement (loAt ∪ lowValueKeysAt) in logit-diff value refinement. q-only debug on gpt2_small_seq32 L9H6 still shows epsAt=1 (refined epsAt=1).","created_at":"2026-01-17T01:06:55Z"},{"id":112,"issue_id":"nfp-1zd","author":"TheDarkchip","text":"Added --prev-shift support (shifted prev/active). q-only debug on gpt2_rigorous_seq32 L0H5 (zero-based) period=16 with --prev-shift: prev=16, epsAt=1, refined epsAt=1. No improvement yet.","created_at":"2026-01-17T01:29:16Z"}]}
{"id":"nfp-1zd.1","title":"State change: mode → in_progress","description":"Set mode to in_progress\n\nReason: Continuing from nfp-0wx: investigate negative logitDiffLB","status":"closed","priority":4,"issue_type":"event","created_at":"2026-01-16T05:40:28.620766+01:00","created_by":"TheDarkchip","updated_at":"2026-01-16T18:10:20.665258+01:00","closed_at":"2026-01-16T18:10:20.665258+01:00","close_reason":"event marker","dependencies":[{"issue_id":"nfp-1zd.1","depends_on_id":"nfp-1zd","type":"parent-child","created_at":"2026-01-16T05:40:28.621434+01:00","created_by":"TheDarkchip"}]}
{"id":"nfp-1zd.2","title":"Sweep top-weight refinement to reduce epsAt","description":"Add or extend debug output to show top weightBoundAt values for argmin q, then sweep top-weight cap/threshold (or budget scaling) to see if epsAt drops below 1. Use gpt2_small_seq32 l9 h6 preset fast as baseline. Record whether epsAt \u003c 1 is achievable without exploding runtime.","acceptance_criteria":"On gpt2_small_seq32 l9 h6: early-exit debug shows epsAt \u003c 1 for argmin q with a documented top-weight cap/threshold, or a documented negative result with supporting data.","status":"open","priority":2,"issue_type":"task","owner":"robin.gieseke@me.com","created_at":"2026-01-16T14:26:39.054841+01:00","created_by":"TheDarkchip","updated_at":"2026-01-16T14:26:39.054841+01:00","dependencies":[{"issue_id":"nfp-1zd.2","depends_on_id":"nfp-1zd","type":"parent-child","created_at":"2026-01-16T14:26:39.055625+01:00","created_by":"TheDarkchip"}]}
{"id":"nfp-1zd.3","title":"Lift loAt via value refinement at minimizer keys","description":"Refine value lower bounds for keys achieving loAt (and/or top-weight keys) using a budget/scale sweep to raise loAt for the argmin q. Evaluate impact on lb0 when epsAt remains 1. Keep refinement bounded to avoid head-build regressions.","acceptance_criteria":"On gpt2_small_seq32 l9 h6: debug shows higher loAt (or improved lb0) after value refinement; if not achievable, document measured deltas and runtime impact.","status":"open","priority":2,"issue_type":"task","owner":"robin.gieseke@me.com","created_at":"2026-01-16T14:26:44.626954+01:00","created_by":"TheDarkchip","updated_at":"2026-01-16T14:26:44.626954+01:00","dependencies":[{"issue_id":"nfp-1zd.3","depends_on_id":"nfp-1zd","type":"parent-child","created_at":"2026-01-16T14:26:44.629685+01:00","created_by":"TheDarkchip"},{"issue_id":"nfp-1zd.3","depends_on_id":"nfp-1zd.2","type":"blocks","created_at":"2026-01-16T14:26:56.186444+01:00","created_by":"TheDarkchip"}],"comments":[{"id":110,"issue_id":"nfp-1zd.3","author":"TheDarkchip","text":"Implemented low-value key refinement in logitDiffLowerBoundRefineOnDemand (loAt keys ∪ lowValueKeysAt with refineLowValueCount). Built Refine/LogitDiff/RefineSound OK. q-only debug on gpt2_small_seq32 L9H6 (NFP_LOGITDIFF_Q_ONLY=31, REFINE=1, VALS=1, EARLY_EXIT=1) still reports epsAt=1 and refined epsAt=1; loAt/valsPrevLo unchanged. No improvement observed so far.","created_at":"2026-01-17T01:06:51Z"}]}
{"id":"nfp-1zd.4","title":"Investigate structural epsAt tightening (beyond per-key bounds)","description":"Look for structural constraints (softmax/row-stochastic) that yield a tighter bound on sum of weights or epsAt without refining every key. Prototype a lemma/bound or explain why it is infeasible in current framework.","acceptance_criteria":"A concrete candidate bound with a Lean lemma/prototype, or a documented reason it cannot improve epsAt in this setting.","notes":"Defer for literature-parity scope: comments indicate no structural sparsity to reduce epsAt under current semantics (finite mask, full key set). Revisit only if mask=-∞ or active-set restriction added.","status":"deferred","priority":3,"issue_type":"task","owner":"robin.gieseke@me.com","created_at":"2026-01-16T14:26:50.183448+01:00","created_by":"TheDarkchip","updated_at":"2026-01-16T18:14:40.779108+01:00","dependencies":[{"issue_id":"nfp-1zd.4","depends_on_id":"nfp-1zd","type":"parent-child","created_at":"2026-01-16T14:26:50.186115+01:00","created_by":"TheDarkchip"},{"issue_id":"nfp-1zd.4","depends_on_id":"nfp-1zd.3","type":"blocks","created_at":"2026-01-16T14:26:59.711766+01:00","created_by":"TheDarkchip"}],"comments":[{"id":92,"issue_id":"nfp-1zd.4","author":"TheDarkchip","text":"Checked structure against attention: otherKeys is univ \\ prev; attention DAG has all keys as parents for each query; causal mask only excludes future keys. For q=31 (argmin) there are no masked keys, so structural support doesn’t reduce the sum in epsAt. Unless we change semantics (mask = -∞) or active set excludes last token, structural epsAt tightening via sparsity seems blocked for this case.","created_at":"2026-01-16T13:50:48Z"}]}
{"id":"nfp-1zd.5","title":"Assess formalization advances for tighter logit-diff bounds","description":"Current state: Mixer only enforces row-stochasticity (no explicit support/sparsity structure); LocalSystem has DAG support and eval, but toMixer does not transport sparsity as a lemma and there is no lemma tying eval to Mixer push/composition. Investigate whether adding structural lemmas (support-based sums, sparse mixers, LocalSystem→Mixer links) can yield tighter epsAt/loAt bounds or prove they cannot affect induction-head logit-diff without new assumptions.","acceptance_criteria":"Either: (a) a concrete proposed lemma/refactor with target module and expected bound improvement, or (b) a documented rationale that formalization advances alone cannot tighten epsAt/loAt for this case.","notes":"Defer: literature-parity focus; without new modeling assumptions, Mixer/LocalSystem structural lemmas won't tighten epsAt for induction heads (see comments). Revisit only if semantics change.","status":"deferred","priority":3,"issue_type":"task","owner":"robin.gieseke@me.com","created_at":"2026-01-16T14:29:08.79176+01:00","created_by":"TheDarkchip","updated_at":"2026-01-16T18:14:44.014661+01:00","dependencies":[{"issue_id":"nfp-1zd.5","depends_on_id":"nfp-1zd","type":"parent-child","created_at":"2026-01-16T14:29:08.792511+01:00","created_by":"TheDarkchip"},{"issue_id":"nfp-1zd.5","depends_on_id":"nfp-1zd.4","type":"blocks","created_at":"2026-01-16T14:29:13.405803+01:00","created_by":"TheDarkchip"}],"comments":[{"id":89,"issue_id":"nfp-1zd.5","author":"TheDarkchip","text":"Impact note: current induction-head bounds are derived from attention weights and value intervals without using Mixer/LocalSystem abstractions, so structural lemmas will only help if we can reinterpret those weights as a (possibly masked) mixer with explicit support. If attention masking or DAG-like sparsity can be formalized (e.g., causal mask, locality), support lemmas could shrink epsAt by zeroing many weights; otherwise Mixer/LocalSystem improvements may not affect logit-diff bounds in this path.","created_at":"2026-01-16T13:34:14Z"},{"id":91,"issue_id":"nfp-1zd.5","author":"TheDarkchip","text":"Quick structural check: attention DAG is fully connected per query (weights depend on all keys), and LocalSystem/Mixer are currently unused in induction bounds. Causal mask only zeroes scores when q\u003ck; for the failing case q=31 there are no masked keys, so no structural sparsity to shrink otherKeys. MaskValue is finite (-10000), so even masked entries are not exact zeros in softmax. Conclusion: without new modeling assumptions (e.g. mask as -∞ or restricting active queries), structural lemmas alone won’t reduce epsAt for the current argmin.","created_at":"2026-01-16T13:50:43Z"}]}
{"id":"nfp-1zd.5.1","title":"Formalize Mixer support/sparsity lemmas","description":"Add lemmas expressing support/sparsity for mixers derived from LocalSystem: weights vanish off parents, and row sums can be restricted to parents. Optionally introduce a sparse-mixer wrapper if needed. Goal is to make structural zeros explicit and usable by downstream bounds.","acceptance_criteria":"Lemmas in Nfp/Mixer.Basic or Nfp/System/LocalSystem show: (1) toMixer weights are 0 off the DAG relation, and (2) row sum over parents equals 1. If a new sparse-mixer structure is introduced, it has basic constructors and support lemmas.","status":"closed","priority":3,"issue_type":"task","owner":"robin.gieseke@me.com","created_at":"2026-01-16T14:32:44.275696+01:00","created_by":"TheDarkchip","updated_at":"2026-01-16T18:14:33.491266+01:00","closed_at":"2026-01-16T18:14:33.491266+01:00","close_reason":"Completed: LocalSystem mixer support/sparsity lemmas added (per comment).","dependencies":[{"issue_id":"nfp-1zd.5.1","depends_on_id":"nfp-1zd.5","type":"parent-child","created_at":"2026-01-16T14:32:44.277834+01:00","created_by":"TheDarkchip"}],"comments":[{"id":90,"issue_id":"nfp-1zd.5.1","author":"TheDarkchip","text":"Added LocalSystem lemmas: toMixer_weight_eq_zero_of_not_parent (support preserved) and row_sum_parents (sum over parents equals 1 using support). Builds: lake build Nfp.System.LocalSystem --wfail, lake build --wfail.","created_at":"2026-01-16T13:41:19Z"}]}
{"id":"nfp-1zd.5.2","title":"Relate LocalSystem.eval to mixer push/composition","description":"Prove a lemma connecting LocalSystem.eval (DAG fixpoint) to a mixer-based view, e.g., eval as repeated application of a mixer/linear operator or a closed-form sum over paths. Use existing LocalSystem.eval_eq and toMixer. This may enable structural bounds or reuse of mixer lemmas.","acceptance_criteria":"A lemma (or short chain of lemmas) in Nfp/System/LocalSystem that connects eval to mixer push/composition or an equivalent linear operator formulation, with clear statements usable by bound proofs.","notes":"Deferred alongside nfp-1zd.5 pending new semantics or evidence of structural sparsity impact.","status":"deferred","priority":3,"issue_type":"task","owner":"robin.gieseke@me.com","created_at":"2026-01-16T14:32:50.767096+01:00","created_by":"TheDarkchip","updated_at":"2026-01-16T18:14:47.943448+01:00","dependencies":[{"issue_id":"nfp-1zd.5.2","depends_on_id":"nfp-1zd.5","type":"parent-child","created_at":"2026-01-16T14:32:50.769595+01:00","created_by":"TheDarkchip"},{"issue_id":"nfp-1zd.5.2","depends_on_id":"nfp-1zd.5.1","type":"blocks","created_at":"2026-01-16T14:33:03.602099+01:00","created_by":"TheDarkchip"}]}
{"id":"nfp-1zd.5.3","title":"Apply structural sparsity to epsAt/loAt bounds","description":"Use the new support/sparsity lemmas to tighten epsAt or loAt bounds: e.g., restrict sums to parents or show certain weightBoundAt entries are provably 0 by structure. Connect these lemmas to logit-diff refinement or one-hot bounds as appropriate.","acceptance_criteria":"A documented path or lemma that uses structural sparsity to reduce epsAt or increase loAt in the logit-diff bound; or a clear negative result showing no improvement for induction-head weights.","notes":"Deferred alongside nfp-1zd.5; structural sparsity currently provides no epsAt/loAt improvement.","status":"deferred","priority":3,"issue_type":"task","owner":"robin.gieseke@me.com","created_at":"2026-01-16T14:32:58.278312+01:00","created_by":"TheDarkchip","updated_at":"2026-01-16T18:14:52.361523+01:00","dependencies":[{"issue_id":"nfp-1zd.5.3","depends_on_id":"nfp-1zd.5","type":"parent-child","created_at":"2026-01-16T14:32:58.280966+01:00","created_by":"TheDarkchip"},{"issue_id":"nfp-1zd.5.3","depends_on_id":"nfp-1zd.5.2","type":"blocks","created_at":"2026-01-16T14:33:09.099954+01:00","created_by":"TheDarkchip"}]}
{"id":"nfp-2cr","title":"Improve GPT-2 induction logit-diff bounds","description":" fails with logitDiffLB -2255815196141/250000250000 below min 0. Need improved value-range bounds or direction data to make logit-diff non-vacuous for canonical head.","status":"in_progress","priority":2,"issue_type":"task","assignee":"TheDarkchip","owner":"robin.gieseke@me.com","created_at":"2026-01-13T17:04:39.844935+01:00","created_by":"TheDarkchip","updated_at":"2026-01-14T02:05:13.230782+01:00","labels":["bug"],"dependencies":[{"issue_id":"nfp-2cr","depends_on_id":"nfp-axz","type":"relates-to","created_at":"2026-01-14T01:54:50.129974+01:00","created_by":"TheDarkchip"},{"issue_id":"nfp-2cr","depends_on_id":"nfp-866","type":"relates-to","created_at":"2026-01-14T01:55:06.203554+01:00","created_by":"TheDarkchip"}],"comments":[{"id":13,"issue_id":"nfp-2cr","author":"TheDarkchip","text":"Starting work on improving logit-diff bounds / direction handling for GPT-2 induction cert.","created_at":"2026-01-13T16:23:08Z"},{"id":14,"issue_id":"nfp-2cr","author":"TheDarkchip","text":"Work interrupted; investigation and proof changes need to be started anew (pending per-query value cert + logit-diff bounds).","created_at":"2026-01-13T16:54:34Z"},{"id":28,"issue_id":"nfp-2cr","author":"TheDarkchip","text":"Mechanistic framing: logit-diff bound remains vacuous for GPT-2 induction heads; likely needs tighter value-range bounds or direction data (OV path) and may interact with residual interval tightening (QK/OV composition).","created_at":"2026-01-14T00:52:32Z"},{"id":29,"issue_id":"nfp-2cr","author":"TheDarkchip","text":"Started fix: clamped per-key weight bounds by epsAt in induction cert builder, added verified logitDiffLowerBoundFromCertBest (max eps/weighted), and used it in head certify path. Targeted build: Build completed successfully (1900 jobs). succeeded.","created_at":"2026-01-14T01:44:29Z"},{"id":30,"issue_id":"nfp-2cr","author":"TheDarkchip","text":"Ran certify_nonvacuous on reports/gpt2_small_seq32_L9H6_t2_n5.head with --max-eps 1; logitDiffLB positive, bound=eps.","created_at":"2026-01-14T07:11:07Z"}]}
{"id":"nfp-2th","title":"Add references section for literature alignment","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-17T20:10:41.926594+01:00","updated_at":"2026-01-17T20:12:33.416567+01:00","closed_at":"2026-01-17T20:12:33.416567+01:00","close_reason":"Added references sections to docs and README with literature citations."}
{"id":"nfp-35g","title":"Verify prev mapping corresponds to induction pattern","description":"Add a proof/verified derivation that prev indices align with the intended prefix-matching induction pattern for a given token sequence, or surface it as an explicit external assumption in certificates.","status":"closed","priority":2,"issue_type":"task","owner":"robin.gieseke@me.com","created_at":"2026-01-13T08:09:29.898676+01:00","created_by":"TheDarkchip","updated_at":"2026-01-13T09:28:30.277766+01:00","closed_at":"2026-01-13T09:28:30.277766+01:00","close_reason":"Closed","comments":[{"id":4,"issue_id":"nfp-35g","author":"TheDarkchip","text":"Added prevOfTokens_spec_of_active lemma in Nfp/Model/InductionPrompt.lean; build passes.","created_at":"2026-01-13T07:25:58Z"},{"id":5,"issue_id":"nfp-35g","author":"TheDarkchip","text":"Added buildInductionHeadInputs_prev_spec_of_active in Nfp/IO/NfptPure.lean (using prevOfTokens_spec_of_active) so model-derived prev is the maximal prior match when period?=none; docs updated.","created_at":"2026-01-13T08:28:27Z"}]}
{"id":"nfp-3cw","title":"Centralize softmax weight invariants in induction stack","description":"## Context\n- The induction-head stack repeatedly re-proves softmax weight nonnegativity and sum-to-one.\n- Examples include: `Nfp/Sound/Induction/OneHot.lean`, `Nfp/Sound/Induction/CoreSound/Basic.lean`,\n  `Nfp/Sound/Bounds/Attention.lean`, and `Nfp/Sound/Induction/LogitDiff.lean`.\n- The existing `Nfp/Prob` + `Nfp/Mixer` formalization is currently unused downstream.\n\n## Problem\n- Proof obligations for `softmax_nonneg` and `softmax_sum_one` are duplicated across multiple modules.\n- This increases proof size, makes invariants harder to see, and raises maintenance cost.\n\n## Goal\n- Centralize the \"softmax yields a probability vector\" facts (nonneg + sum-to-one) so downstream\n  proofs can reuse them without repeating the same boilerplate.\n\n## Scope / Proposed Approach\n- Introduce a small wrapper structure (e.g., `SoftmaxWeights` or `ProbVecReal`) that packages:\n  - `weights : Fin seq -\u003e Fin seq -\u003e Real`\n  - `nonneg : forall q k, 0 \u003c= weights q k`\n  - `sum_one : forall q, (sum k, weights q k) = 1`\n- Add a lemma that constructs the wrapper from `Circuit.softmax` with a short docstring.\n- Update induction-head proofs to consume the wrapper instead of re-proving nonneg/sum-one locally.\n  Target files:\n  - `Nfp/Sound/Induction/OneHot.lean`\n  - `Nfp/Sound/Induction/CoreSound/Basic.lean`\n  - `Nfp/Sound/Bounds/Attention.lean`\n  - `Nfp/Sound/Induction/LogitDiff.lean`\n- Optionally evaluate whether `Nfp.Prob.ProbVec` can be generalized to Real; if the NNReal\n  mismatch is too invasive, keep the wrapper separate for now and document the tradeoff.\n\n## Acceptance Criteria\n- A new wrapper + constructor lemma exists with docstrings and no linter disables.\n- The target files no longer re-prove `Circuit.softmax_nonneg` and `Circuit.softmax_sum_one` locally;\n  they use the wrapper instead.\n- `lake build --wfail` succeeds.\n- No new axioms are introduced.\n\n## Risks / Notes\n- `ProbVec` is NNReal-based while softmax outputs Real; bridging may require a new structure or\n  a carefully scoped coercion. Avoid global simp rule growth.\n","acceptance_criteria":"- Wrapper + constructor lemma with docstrings added\n- Target induction/attention modules reuse wrapper (no local softmax nonneg/sum-one proofs)\n- `lake build --wfail` passes\n- No new axioms or linter disables","status":"closed","priority":2,"issue_type":"task","assignee":"TheDarkchip","owner":"robin.gieseke@me.com","created_at":"2026-01-14T01:21:43.85933+01:00","created_by":"TheDarkchip","updated_at":"2026-01-14T01:40:34.732005+01:00","closed_at":"2026-01-14T01:40:34.732005+01:00","close_reason":"completed","comments":[{"id":26,"issue_id":"nfp-3cw","author":"TheDarkchip","text":"Implemented Circuit.SoftmaxWeights wrapper + softmaxWeights constructor and softmaxWeights_weights lemma. Refactored OneHot, CoreSound/Basic, and Bounds/Attention to use wrapper for nonneg/sum_one (no local softmax_nonneg/sum_one). lake build --wfail succeeded.","created_at":"2026-01-14T00:39:46Z"}]}
{"id":"nfp-4j1","title":"Add circuit+copy induction scoring","description":"Extend discovery script with circuit_copy mode combining prev-token attention, stripe attention, and OV copy score for literature parity.","notes":"Implemented circuit_copy scoring and ran 15-seed parity scan; top avg pair prev=L5H12 ind=L8H3, then L8H2, L10H10.","status":"closed","priority":2,"issue_type":"task","owner":"robin.gieseke@me.com","created_at":"2026-01-16T22:32:22.131513+01:00","created_by":"TheDarkchip","updated_at":"2026-01-16T22:32:25.92955+01:00","closed_at":"2026-01-16T22:32:25.929554+01:00"}
{"id":"nfp-4y8","title":"Refactor parallel reductions to sequential core with equivalence lemmas","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-13T01:42:58.138743+01:00","updated_at":"2026-01-13T04:06:02.594691+01:00","closed_at":"2026-01-13T04:06:02.594691+01:00","close_reason":"Closed"}
{"id":"nfp-4z9","title":"Add trusted parser for NFP model slices","description":"Verified parser for WQ/WK/WV/WO, biases, LN params, and unembedding so QK/OV certificates rely on trusted extraction.","acceptance_criteria":"Parser returns proof-carrying slices; CLI uses it by default; lake build --wfail passes.","status":"open","priority":2,"issue_type":"task","owner":"robin.gieseke@me.com","created_at":"2026-01-14T01:51:29.014419+01:00","created_by":"TheDarkchip","updated_at":"2026-01-14T01:51:29.014419+01:00","dependencies":[{"issue_id":"nfp-4z9","depends_on_id":"nfp-mzt","type":"relates-to","created_at":"2026-01-14T01:54:59.550956+01:00","created_by":"TheDarkchip"},{"issue_id":"nfp-4z9","depends_on_id":"nfp-axz","type":"relates-to","created_at":"2026-01-14T01:55:02.927515+01:00","created_by":"TheDarkchip"}]}
{"id":"nfp-50q","title":"Add careful statement of certificate utility","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-17T20:06:57.159326+01:00","updated_at":"2026-01-17T20:07:23.395588+01:00","closed_at":"2026-01-17T20:07:23.395588+01:00","close_reason":"Added docs/cert_usefulness.md and linked from README."}
{"id":"nfp-58o","title":"Adopt 1-based indexing for certificate payloads","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-17T10:04:55.366051+01:00","updated_at":"2026-01-17T10:04:55.431613+01:00","closed_at":"2026-01-17T10:04:55.431613+01:00","close_reason":"Implemented 1-based payload parsing/output and updated docs."}
{"id":"nfp-5gn","title":"Refactor module structure across repo to mathlib-style headers/imports","description":"Migrate all Lean files away from experimental module headers and public imports/sections; align import structure with mathlib-style source files.","status":"closed","priority":2,"issue_type":"task","assignee":"TheDarkchip","owner":"robin.gieseke@me.com","created_at":"2026-01-14T00:34:55.171544+01:00","created_by":"TheDarkchip","updated_at":"2026-01-14T00:59:02.052937+01:00","closed_at":"2026-01-14T00:59:02.052937+01:00","close_reason":"Closed","labels":["refactor"],"comments":[{"id":24,"issue_id":"nfp-5gn","author":"TheDarkchip","text":"Added mathlib module structure style guide to AGENTS.md; full repo refactor still pending.","created_at":"2026-01-13T23:41:15Z"},{"id":25,"issue_id":"nfp-5gn","author":"TheDarkchip","text":"Rescanned local mathlib: module headers in all .lean files; public import common but not universal; public section often with @[expose], plus public meta/noncomputable sections and rare import all. Updated AGENTS.md style guide accordingly.","created_at":"2026-01-13T23:45:41Z"}]}
{"id":"nfp-5lg","title":"Add direction search report for non-vacuous certs","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-17T10:41:12.06651+01:00","updated_at":"2026-01-17T10:41:12.131904+01:00","closed_at":"2026-01-17T10:41:12.131904+01:00","close_reason":"Added report flag/top-k to untrusted direction search and documented usage."}
{"id":"nfp-60c","title":"Add causal-mask support to circuit-layer attention core","description":"Circuit-layer attention core (Nfp/Circuit/Layers/Attention.lean) computes scores and softmax without a causal mask. For GPT-2 literature parity, add an optional mask predicate (q\u003ck) that either zeroes masked weights or injects a maskValue before softmax, and thread it through attentionGate/attentionCircuit/attentionTyped. This aligns circuit semantics with causal attention when these circuits are used.","acceptance_criteria":"- attention core supports a causal mask option; - masked weights are zero or follow a documented maskValue; - docs clarify unmasked vs causal variants.","status":"open","priority":3,"issue_type":"task","owner":"robin.gieseke@me.com","created_at":"2026-01-16T18:26:01.980788+01:00","created_by":"TheDarkchip","updated_at":"2026-01-16T18:26:01.980788+01:00"}
{"id":"nfp-6o4","title":"Move induction cert generation/heavy bounds out of trusted Sound","description":"Summary\n- Induction-head certificate generation and bound computation live in trusted `Nfp.Sound.*` modules with task-based parallelism and split heuristics.\n\nViolation\n- The trusted side should be a small checker over an explicit cert; instead it performs heavy compute (bounds, caches, search/split heuristics) and constructs the cert internally.\n\nEvidence\n- `Nfp/Sound/Induction/Core/Basic.lean`:\n  - `buildInductionHeadCoreCacheWith`, `buildInductionCertFromHeadCoreWith?`, `buildInductionCertFromHeadCore?` compute certs from raw inputs.\n  - Uses `InductionHeadSplitConfig` budgets and task-based caching.\n- `Nfp/Sound/Induction/HeadBounds/Basic.lean`:\n  - Task graphs (`Task.spawn`) for bound reductions and dot products.\n- `Nfp/Sound/Induction/CoreSound/Basic/CacheBounds.lean` and `Nfp/Sound/Induction/CoreSound/Basic/CertSound.lean`:\n  - Parallel cache construction for LayerNorm/QKV/dot-product bounds.\n- `Nfp/Sound/Induction/HeadOutput.lean`:\n  - `buildInductionCertFromHeadWith?` / `buildHeadOutputIntervalFromHead?` derive certs directly from head inputs.\n\nImpact\n- Trust boundary blurred; verification is no longer a small checker and involves heavy compute/heuristics in trusted code.\n- Makes external certificate generation (outside Lean) impossible without re-implementing these derivations.\n\nProposed Fix\n- Move bound computation and cert assembly into untrusted modules (Lean or external tooling).\n- Keep a minimal trusted checker that validates an explicit induction certificate (see nfp-9pf).\n- Adjust IO/CLI paths (`Nfp/IO/InductionHead/*.lean`) to accept explicit certs and only call the checker.\n\nDependencies / Blockers\n- Blocked by: nfp-9pf (need explicit cert schema + checker).\n- May require revisiting how split heuristics are represented/witnessed in certs.\n","notes":"Removed induction cert generators/refinement/cache modules from Nfp.Sound.Induction (deleted Core/Basic, CoreSound, Refine*, HeadBounds; pared HeadOutput/LogitDiff to explicit-cert reasoning). Core now re-exports CoreDefs only.","status":"closed","priority":2,"issue_type":"task","owner":"robin.gieseke@me.com","created_at":"2026-01-17T07:00:18.825723+01:00","created_by":"TheDarkchip","updated_at":"2026-01-17T08:07:07.856965+01:00","closed_at":"2026-01-17T08:07:07.856967+01:00","dependencies":[{"issue_id":"nfp-6o4","depends_on_id":"nfp-9pf","type":"blocks","created_at":"2026-01-17T07:00:18.829041+01:00","created_by":"TheDarkchip"}]}
{"id":"nfp-7al","title":"Remove Lean raw-input parsers/loaders (cert-only IO)","description":"Lean still ships raw-input parsers/loaders (softmax-margin/value-range raw inputs, downstream matrix payloads, induction-head input files). These support Lean-side witness generation from raw inputs, which conflicts with the Python-only generation split. Remove raw parsers/loaders and update IO.Parse aggregators to be cert-only.","acceptance_criteria":"- Raw parsers/loaders removed: SoftmaxMarginRaw/ValueRangeRaw, DownstreamMatrixRaw, InductionHeadInputs parse\\n- IO.Parse aggregators no longer import removed modules\\n- Nfp/IO/Loaders.lean only loads certificates\\n- lake build --wfail passes","status":"closed","priority":2,"issue_type":"task","owner":"robin.gieseke@me.com","created_at":"2026-01-17T08:43:51.696726+01:00","created_by":"TheDarkchip","updated_at":"2026-01-17T09:02:00.724055+01:00","closed_at":"2026-01-17T09:02:00.724055+01:00","close_reason":"Closed","labels":["io","split"]}
{"id":"nfp-7ky","title":"Update README for literature alignment and direction report","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-17T10:49:14.470493+01:00","updated_at":"2026-01-17T10:49:14.541966+01:00","closed_at":"2026-01-17T10:49:14.541966+01:00","close_reason":"Added literature alignment note, direction report format, and dependency list."}
{"id":"nfp-7p1","title":"Add optional token/prev/active verification in induction cert CLI","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-17T10:55:33.753149+01:00","updated_at":"2026-01-17T11:06:35.163154+01:00","closed_at":"2026-01-17T11:06:35.163154+01:00","close_reason":"Added optional token list verification for prev/active and generator token output."}
{"id":"nfp-853","title":"Remove all .nfpt format dependencies","description":"Eliminate all references to the .nfpt/NFP_BINARY formats across scripts, tests, and docs; remove helper scripts and fixtures that rely on them.","acceptance_criteria":"- No .nfpt/NFP_BINARY/NFP_TEXT references remain in repo\\n- Scripts and fixtures using .nfpt removed\\n- README/CLAIMS updated","status":"closed","priority":2,"issue_type":"task","owner":"robin.gieseke@me.com","created_at":"2026-01-17T09:34:43.199253+01:00","created_by":"TheDarkchip","updated_at":"2026-01-17T09:34:49.782175+01:00","closed_at":"2026-01-17T09:34:49.782175+01:00","close_reason":"Closed","labels":["cleanup","parity"]}
{"id":"nfp-866","title":"Remove perfect-square head_dim restriction for attention scaling","description":"certify_head_model currently requires head_dim to be a perfect square to represent 1/sqrt(d) scaling exactly. This distorts QK margin bounds when head_dim is not square. Add a rational/interval bound for the scale (or exact sqrtLower/sqrtUpper) with proofs.","acceptance_criteria":"Supports non-square head_dim with bounded scale factor; softmax-margin proofs updated; lake build --wfail passes.","status":"open","priority":2,"issue_type":"task","owner":"robin.gieseke@me.com","created_at":"2026-01-14T01:51:49.18715+01:00","created_by":"TheDarkchip","updated_at":"2026-01-14T01:52:04.455932+01:00","dependencies":[{"issue_id":"nfp-866","depends_on_id":"nfp-2cr","type":"relates-to","created_at":"2026-01-14T01:55:06.203015+01:00","created_by":"TheDarkchip"}]}
{"id":"nfp-87s","title":"Model causal mask as hard exclusion (softmax over unmasked keys)","description":"Current induction proofs treat causal masking via a finite maskValue (-10000) inside scoresRealOfInputs, so masked keys still contribute nonzero softmax mass. Literature and standard transformer definitions treat causal mask as -∞ (exact exclusion). Add a variant (or flag) that models a hard mask: softmax over unmasked keys only, with exact zeros for masked keys. Update head bounds/refinement and cert soundness to use the hard-mask variant when requested, and document the semantic difference.","acceptance_criteria":"- new hard-mask semantics implemented and wired through induction bounds; - proofs updated to show masked weights are exactly 0; - docs/CLI clarify finite vs hard mask assumptions.","status":"open","priority":2,"issue_type":"task","owner":"robin.gieseke@me.com","created_at":"2026-01-16T18:19:35.322475+01:00","created_by":"TheDarkchip","updated_at":"2026-01-16T18:19:35.322475+01:00"}
{"id":"nfp-886","title":"Move GPT-2 head input builders out of trusted Sound","description":"The Sound namespace should not construct inputs from raw model slices. \"buildInductionHeadInputs\" and \"buildInductionHeadInputsShift\" in Nfp.Sound.Gpt2.HeadInputs construct InductionHeadInputs from GPT-2 slices and prompt periods.","notes":"Moved to untrusted Nfp.Gpt2.HeadInputs and removed from Nfp.Sound reexports.","status":"closed","priority":2,"issue_type":"task","owner":"robin.gieseke@me.com","created_at":"2026-01-17T08:21:59.225103+01:00","created_by":"TheDarkchip","updated_at":"2026-01-17T08:22:04.677076+01:00","closed_at":"2026-01-17T08:22:04.677078+01:00"}
{"id":"nfp-8or","title":"Formalize or scope induction head inputs vs true layer residuals","description":"Current Lean head inputs are built from embeddings (Nfp/IO/NfptPure.lean), not the true layer residual stream. This is fine for head-level certificates, but it does not match the canonical induction-head definition (which is about actual model activations). Add an explicit lemma/spec or module documenting the scope, or extend the formalization to accept externally provided layer-residual inputs with a clear trust boundary.\\n\\nAcceptance criteria:\\n- explicit docstring/lemma clarifying the proxy nature of embedding-derived head inputs, or a new input variant for true residuals.\\n- updated audit doc to reflect the scope.\\n\\nDepends on:\\n- nfp-mzt (bridge to block/model logits) if we choose the residual-input extension route.","status":"open","priority":2,"issue_type":"task","owner":"robin.gieseke@me.com","created_at":"2026-01-16T21:32:16.614396+01:00","created_by":"TheDarkchip","updated_at":"2026-01-16T21:32:16.614396+01:00"}
{"id":"nfp-96w","title":"Tighten induction-head bounds for nonvacuous certification","description":"Upgrade per-query epsilon bounds to reduce vacuity in certify_head_model_nonvacuous.","status":"closed","priority":2,"issue_type":"task","owner":"robin.gieseke@me.com","created_at":"2026-01-12T16:51:54.831367+01:00","created_by":"TheDarkchip","updated_at":"2026-01-12T17:12:14.022297+01:00","closed_at":"2026-01-12T17:12:14.022297+01:00","close_reason":"Implemented per-key epsilon bounds and proof updates; builds pass."}
{"id":"nfp-9pf","title":"Define explicit InductionHead cert format + checker","description":"Summary\n- InductionHead certificates are currently only produced via trusted builders with proofs; there is no small, explicit checker over a serialized certificate.\n\nViolation\n- Intended split is untrusted generation vs trusted verification. Today there is no trusted *checker* for `InductionHeadCert` that can validate an explicit certificate produced elsewhere.\n\nEvidence\n- `Nfp/Sound/Induction/CoreDefs.lean` defines `InductionHeadCert` and `InductionHeadCertSound` as a Prop, but no `checkInductionHeadCert`-style function exists.\n- `Nfp/Sound/Induction/HeadOutput.lean` and `Nfp/Sound/Induction/Core/Basic.lean` only expose builder APIs returning proofs.\n\nImpact\n- Untrusted generation cannot be separated cleanly; the trusted kernel must compute the cert to prove soundness, which violates the intended boundary and makes verification nontrivial/heavy.\n\nProposed Fix\n- Specify an explicit serialized cert schema for induction head (fields, bounds, any precomputed witnesses).\n- Add a small boolean checker (or proof-carrying checker) in a trusted module (ideally `Nfp.Circuit.Cert.*`) with a `sound` theorem.\n- Refactor proof obligations to use the checker result rather than recomputing bounds.\n\nDependencies / Blockers\n- Requires agreement on the certificate schema (what fields/witnesses are sufficient and minimal).\n- Likely blocks moving any induction cert generation out of trusted code.\n","status":"closed","priority":2,"issue_type":"task","owner":"robin.gieseke@me.com","created_at":"2026-01-17T07:00:01.522029+01:00","created_by":"TheDarkchip","updated_at":"2026-01-17T07:13:28.848429+01:00","closed_at":"2026-01-17T07:13:28.848429+01:00","close_reason":"Implemented explicit InductionHead cert schema + checker in Circuit.Cert.InductionHead"}
{"id":"nfp-anj","title":"Make unembedding explicit or prove tie to embeddings","description":"Literature defines logit effect using the unembedding matrix W_U; in Lean, Gpt2HeadSlice.directionVec uses wte (token embeddings) with an implicit tie assumption. Add explicit unembedding columns to Gpt2 slices (or a theorem stating W_U = W_E^T for the models we certify) and update direction derivations accordingly, so logit-diff aligns with literature semantics.","acceptance_criteria":"- unembedding is explicit or a tie assumption is formalized; - direction vector derivations updated; - docs state the logit effect uses W_U as in the literature.","status":"open","priority":2,"issue_type":"task","owner":"robin.gieseke@me.com","created_at":"2026-01-16T18:19:42.477964+01:00","created_by":"TheDarkchip","updated_at":"2026-01-16T18:19:42.477964+01:00"}
{"id":"nfp-axz","title":"Tighten residual interval bounds using attention structure","description":"End-to-end logit-diff bounds are often vacuous because residual intervals ignore attention-score/softmax structure. Use certified attention weights or score-aware bounds to tighten residual propagation and preserve mechanistic QK/OV contributions.","acceptance_criteria":"Residual interval bounds incorporate attention/softmax structure or certified weights; GPT-2 end-to-end bounds improve; lake build --wfail passes.","status":"open","priority":2,"issue_type":"task","owner":"robin.gieseke@me.com","created_at":"2026-01-14T01:51:41.833297+01:00","created_by":"TheDarkchip","updated_at":"2026-01-14T01:51:41.833297+01:00","dependencies":[{"issue_id":"nfp-axz","depends_on_id":"nfp-mzt","type":"relates-to","created_at":"2026-01-14T01:54:44.801444+01:00","created_by":"TheDarkchip"},{"issue_id":"nfp-axz","depends_on_id":"nfp-2cr","type":"relates-to","created_at":"2026-01-14T01:54:50.129397+01:00","created_by":"TheDarkchip"},{"issue_id":"nfp-axz","depends_on_id":"nfp-suu","type":"relates-to","created_at":"2026-01-14T01:54:55.041696+01:00","created_by":"TheDarkchip"},{"issue_id":"nfp-axz","depends_on_id":"nfp-4z9","type":"relates-to","created_at":"2026-01-14T01:55:02.928061+01:00","created_by":"TheDarkchip"}]}
{"id":"nfp-bc5","title":"Fix activation discovery head splitting for transformers","description":"Activation mode uses GPT2Attention._split_heads which is absent in recent transformers; replace with local reshape/permute helper.","notes":"Replaced GPT2Attention._split_heads with local reshape/permute helper so activation discovery works on recent transformers.","status":"closed","priority":2,"issue_type":"bug","assignee":"TheDarkchip","owner":"robin.gieseke@me.com","created_at":"2026-01-16T21:44:49.332958+01:00","created_by":"TheDarkchip","updated_at":"2026-01-16T21:45:30.603473+01:00","closed_at":"2026-01-16T21:45:30.603476+01:00"}
{"id":"nfp-c57","title":"Aggregate synthetic prefix-matching scores across multiple seeds","description":"Canonical induction-head evaluation averages prefix-matching behavior over many repeated-random prompts. Add a multi-seed sweep mode (or helper) that runs scan_gpt2_induction_sound.py --synthetic across seeds/pattern lengths, aggregates attn/copy scores per head, and reports stability (mean/std or rank frequency).\\n\\nAcceptance criteria:\\n- command-line entry to specify seed list/range for synthetic scans.\\n- aggregate report with mean/median and variability per head.\\n- documentation snippet explaining canonical multi-seed evaluation.\\n\\nDepends on:\\n- nfp-lok.3 (synthetic prefix-matching mode).","notes":"Generated 5 seeds (2026-2030) for pat128 repeated prompts; ran activation attn_copy discovery; averaged scores across seeds; top avg heads: L8H3, L8H2, L4H1, L10H10, L9H2, L11H7...","status":"closed","priority":2,"issue_type":"task","assignee":"TheDarkchip","owner":"robin.gieseke@me.com","created_at":"2026-01-16T21:32:03.552918+01:00","created_by":"TheDarkchip","updated_at":"2026-01-16T22:01:36.298481+01:00","closed_at":"2026-01-16T22:01:36.298483+01:00"}
{"id":"nfp-cbk","title":"Document non-vacuous induction certification example","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-17T10:17:15.294254+01:00","updated_at":"2026-01-17T10:17:15.362318+01:00","closed_at":"2026-01-17T10:17:15.362318+01:00","close_reason":"Added README example for direction metadata and min-logit-diff gate."}
{"id":"nfp-cit","title":"Generalize canonical direction derivation for induction prompts","description":"deriveDirectionFromTokens in IO picks a target/negative based on the last token only. For literature-parity evaluation, define a canonical direction derivation at an explicit query index (or over active queries) using the shifted-prev rule (copy token after previous occurrence). Expose it in a pure module (Model/InductionPrompt) with a lemma about correctness, and let CLI choose q explicitly.","acceptance_criteria":"- pure function derives direction for a specified q using the canonical induction rule; - lemma states relation to tokens/prev mapping; - CLI can select q or uses a documented default.","status":"open","priority":3,"issue_type":"task","owner":"robin.gieseke@me.com","created_at":"2026-01-16T18:19:47.260155+01:00","created_by":"TheDarkchip","updated_at":"2026-01-16T18:19:47.260155+01:00"}
{"id":"nfp-dep","title":"Prove/log derivation for logit-diff direction vector","description":"Add a verified link from DirectionSpec (target/negative ids) to the actual unembedding direction used in certification, or explicitly verify the direction vector against model weights.","status":"closed","priority":2,"issue_type":"task","owner":"robin.gieseke@me.com","created_at":"2026-01-13T08:09:25.330985+01:00","created_by":"TheDarkchip","updated_at":"2026-01-13T09:28:40.824036+01:00","closed_at":"2026-01-13T09:28:40.824036+01:00","close_reason":"Closed","comments":[{"id":6,"issue_id":"nfp-dep","author":"TheDarkchip","text":"Added buildInductionHeadInputs_def/buildInductionHeadInputs_direction_def in Nfp/IO/NfptPure.lean: directionSpec matches target/negative ids and direction is colTarget-colNegative from unembedding columns. Docs now note the remaining assumption that unembedding represents logits.","created_at":"2026-01-13T08:28:36Z"}]}
{"id":"nfp-duj","title":"Move interval bounds out of trusted Sound namespace","description":"Bounds modules under Nfp.Sound.Bounds compute interval bounds directly from model weights. This is untrusted certificate generation logic and should live outside trusted namespaces.","notes":"Moved Nfp.Sound.Bounds to Nfp.Bounds and removed Nfp.Sound reexport.","status":"closed","priority":2,"issue_type":"task","owner":"robin.gieseke@me.com","created_at":"2026-01-17T08:31:07.095852+01:00","created_by":"TheDarkchip","updated_at":"2026-01-17T08:31:13.264269+01:00","closed_at":"2026-01-17T08:31:13.264271+01:00"}
{"id":"nfp-e53","title":"Refactor and optimize module structure","description":"Create a dedicated branch and plan for Lean module/import refactor guided by the Lean manual.","notes":"Pilot module-system conversion for Core/Prob/Mixer/System: added module headers, public sections, public imports for used deps; enabled experimental.module in lakefile.toml and adjusted imports accordingly.","status":"closed","priority":2,"issue_type":"task","assignee":"TheDarkchip","owner":"robin.gieseke@me.com","created_at":"2026-01-13T17:57:19.507102+01:00","created_by":"TheDarkchip","updated_at":"2026-01-13T19:34:39.270845+01:00","closed_at":"2026-01-13T19:34:39.270845+01:00","close_reason":"Closed","comments":[{"id":15,"issue_id":"nfp-e53","author":"TheDarkchip","text":"Converted Nfp/Circuit and Nfp/Model trees to module system: added module headers, public imports, @[expose] public section on definition files, and public-import aggregators. lake build --wfail passes.","created_at":"2026-01-13T18:16:45Z"},{"id":16,"issue_id":"nfp-e53","author":"TheDarkchip","text":"Converted Nfp/Sound and Nfp/IO trees (and Nfp.lean) to module system with public imports; adjusted aggregator modules to avoid empty public sections. Fixed module-system visibility by switching HeadBounds/Basic and Bounds/Transformer/Embedding to public section and making internal spec lemmas private. lake build --wfail passes.","created_at":"2026-01-13T18:27:02Z"},{"id":17,"issue_id":"nfp-e53","author":"TheDarkchip","text":"Switched Nfp/IO modules to public section (no expose) and made IO spec/unfold lemmas private (HeadScore, NfptPure, InductionHead.Bytes). lake build nfp --wfail now passes.","created_at":"2026-01-13T18:32:50Z"},{"id":18,"issue_id":"nfp-e53","author":"TheDarkchip","text":"Converted top-level aggregator modules (Nfp.Circuit/IO/Model/Sound) to module headers with public imports; lake build --wfail rechecked.","created_at":"2026-01-13T18:34:04Z"}]}
{"id":"nfp-ete","title":"Evaluate nonvacuous induction certification on real models","description":"Run certify_head_model_nonvacuous on GPT-2/GPT2-small binaries; if still vacuous, explore tighter value-range bounds (per-query or mask-aware).","notes":"Investigating nonvacuous induction head certification","status":"closed","priority":2,"issue_type":"task","owner":"robin.gieseke@me.com","created_at":"2026-01-12T17:11:56.642685+01:00","created_by":"TheDarkchip","updated_at":"2026-01-12T23:28:32.804466+01:00","closed_at":"2026-01-12T23:28:32.804466+01:00","close_reason":"Closed","comments":[{"id":2,"issue_id":"nfp-ete","author":"TheDarkchip","text":"Fixed f64-\u003eRat conversion: use signed exponent in NfptPure.ratOfFloatBits. Lean model reads now match head inputs; certify_head_model_nonvacuous succeeds (build ~370s, logit diff fast).","created_at":"2026-01-12T22:28:26Z"}]}
{"id":"nfp-g0j","title":"Refactor HeadBounds reduction helpers","description":"Deduplicate chunked reduction task helpers in Nfp/Sound/Induction/HeadBounds.lean to reduce proof churn while preserving behavior.","status":"closed","priority":2,"issue_type":"task","owner":"robin.gieseke@me.com","created_at":"2026-01-12T16:21:11.583135+01:00","created_by":"TheDarkchip","updated_at":"2026-01-12T16:22:20.621327+01:00","closed_at":"2026-01-12T16:22:20.621327+01:00","close_reason":"Completed"}
{"id":"nfp-g78","title":"Refactor CoreSound lemmas for stability","description":"Refactor Nfp/Sound/Induction/CoreSound.lean to reduce proof churn and improve maintainability; keep proofs explicit and stable.","status":"closed","priority":2,"issue_type":"task","owner":"robin.gieseke@me.com","created_at":"2026-01-12T16:12:25.347409+01:00","created_by":"TheDarkchip","updated_at":"2026-01-12T16:16:04.322186+01:00","closed_at":"2026-01-12T16:16:04.322186+01:00","close_reason":"Completed","labels":["in_progress"]}
{"id":"nfp-ge0","title":"Update Python induction-cert generators to emit explicit induction-head cert format","description":"Lean now checks explicit induction-head certificates (eps/eps-at/weight-bound/value intervals in a single file). Current Python generators (build_gpt2_induction_cert*.py) still emit separate softmax/value-range files. Update them to produce the full induction-head cert format expected by Nfp.IO.InductionHead.Cert.","acceptance_criteria":"- build_gpt2_induction_cert.py outputs full induction-head cert (eps-at, weight-bound, val/val-lo/val-hi/lo/hi, optional direction)\\n- build_gpt2_induction_cert_from_binary.py outputs the same single-file cert\\n- Example CLI in README matches new output\\n- lake build --wfail passes","status":"closed","priority":2,"issue_type":"task","owner":"robin.gieseke@me.com","created_at":"2026-01-17T08:50:35.913542+01:00","created_by":"TheDarkchip","updated_at":"2026-01-17T09:02:00.727101+01:00","closed_at":"2026-01-17T09:02:00.727101+01:00","close_reason":"Closed","labels":["python","split"]}
{"id":"nfp-giz","title":"Remove remaining @[expose] and add def lemmas for module system","description":"Drop remaining expose attributes, add explicit def lemmas, and update simp uses to match mathlib-style module boundaries.","status":"closed","priority":2,"issue_type":"task","owner":"robin.gieseke@me.com","created_at":"2026-01-13T21:36:36.153438+01:00","created_by":"TheDarkchip","updated_at":"2026-01-13T22:23:45.005299+01:00","closed_at":"2026-01-13T22:23:45.005299+01:00","close_reason":"Closed","labels":["refactor"]}
{"id":"nfp-goo","title":"Add user-facing induction certification demo","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-17T20:02:48.318238+01:00","updated_at":"2026-01-17T20:03:15.131863+01:00","closed_at":"2026-01-17T20:03:15.131863+01:00","close_reason":"Added docs/demo.md and linked from README."}
{"id":"nfp-hrc","title":"Move raw parsing/transform logic out of trusted IO.Pure/NfptPure","description":"Summary\n- Trusted namespaces `Nfp.IO.Pure.*` and `Nfp.IO.NfptPure` include substantial parsing and data transformation logic for raw inputs and binary model files.\n\nViolation\n- The trusted side should be a small checker; raw parsing and data extraction are untrusted generation steps and currently live inside trusted namespaces.\n\nEvidence\n- `Nfp/IO/Pure/SoftmaxMargin/Raw.lean` / `ValueRange/Raw.lean`: parse raw payloads into data structures.\n- `Nfp/IO/Pure/InductionHead.lean`: parses induction head inputs from bytes.\n- `Nfp/IO/NfptPure.lean`: parses headers and numeric literals, scans binary blobs, and extracts slices.\n\nImpact\n- Trust boundary is muddied; untrusted parsing/decoding lives in trusted namespaces.\n- Hard to argue “small checker” when parsing logic is in the trusted kernel.\n\nProposed Fix\n- Relocate parsing/decoding to untrusted IO modules (or mark as untrusted tooling), and feed explicit certs into trusted checkers.\n- If any parsing must remain trusted, make it return proof-carrying structures or add explicit validation theorems for each pure definition.\n\nDependencies / Blockers\n- Coordinate with CLI changes (e.g., nfp-0z1) so raw parsing is not needed in trusted code paths.\n","notes":"Moved IO.Pure parsing helpers to untrusted Nfp.IO.Parse (renamed modules/namespace), updated IO.Util/Loaders/InductionHead.Cert to use Parse. Removed trusted IO.Pure namespace entirely.","status":"closed","priority":2,"issue_type":"task","owner":"robin.gieseke@me.com","created_at":"2026-01-17T07:00:52.978416+01:00","created_by":"TheDarkchip","updated_at":"2026-01-17T08:17:22.794601+01:00","closed_at":"2026-01-17T08:17:22.794603+01:00"}
{"id":"nfp-inz","title":"Add induction-stripe attention scoring","description":"Extend discover_gpt2_induction_targets.py with stripe-based attention score (period shift) to align with canonical induction stripe evaluation.","notes":"Added --score-mode=stripe with stripe-period, computes stripe stats from attention weights for embedding and activation paths; reports stripe metrics in output/JSON.","status":"closed","priority":2,"issue_type":"task","assignee":"TheDarkchip","owner":"robin.gieseke@me.com","created_at":"2026-01-16T22:03:51.566809+01:00","created_by":"TheDarkchip","updated_at":"2026-01-16T22:07:50.05001+01:00","closed_at":"2026-01-16T22:07:50.050013+01:00"}
{"id":"nfp-jnb","title":"Remove Lean head-input builder (Python-only witness generation)","description":"Nfp/Gpt2/HeadInputs.lean still defines buildInductionHeadInputs* in Lean. Even though untrusted, it contradicts the intended split (witness generation happens in Python). Remove the Lean builder module and any references, leaving only Python scripts to generate head-inputs/certs.","acceptance_criteria":"- Nfp/Gpt2/HeadInputs.lean removed and no remaining references to buildInductionHeadInputs*\\n- Docs no longer cite Lean head-input builders as generation paths\\n- lake build --wfail passes","status":"closed","priority":2,"issue_type":"task","owner":"robin.gieseke@me.com","created_at":"2026-01-17T08:43:46.878187+01:00","created_by":"TheDarkchip","updated_at":"2026-01-17T09:02:00.722336+01:00","closed_at":"2026-01-17T09:02:00.722336+01:00","close_reason":"Closed","labels":["docs","split"]}
{"id":"nfp-k70","title":"Prune non-head-level residual/downstream components for literature parity","description":"Align induction certification with literature parity (Transformer Circuits + Induction Heads) at head-level only. Remove residual/downstream cert modules, end-to-end lemmas, scripts, and docs that exceed head-level scope.","acceptance_criteria":"- Residual/downstream cert modules and parsers removed\\n- End-to-end lemmas/docs removed\\n- Scripts for residual/downstream removed\\n- Builds pass","status":"closed","priority":2,"issue_type":"task","owner":"robin.gieseke@me.com","created_at":"2026-01-17T09:21:52.717806+01:00","created_by":"TheDarkchip","updated_at":"2026-01-17T09:26:53.002255+01:00","closed_at":"2026-01-17T09:26:53.002255+01:00","close_reason":"Closed","labels":["cleanup","parity"]}
{"id":"nfp-lok","title":"Pivot Python induction discovery to attention-based scoring","description":"Switch discovery/scan/sweep scripts to literature-aligned attention scoring (bigram prev) as default. Add score-mode switch (attn vs logit), update outputs/docs, and keep logit mode optional.","status":"open","priority":2,"issue_type":"task","owner":"robin.gieseke@me.com","created_at":"2026-01-16T17:37:00.312683+01:00","created_by":"TheDarkchip","updated_at":"2026-01-16T17:37:00.312683+01:00","labels":["status:in_progress"],"comments":[{"id":97,"issue_id":"nfp-lok","author":"TheDarkchip","text":"Implemented attn-vs-logit scoring switch in discovery/scan/sweep (default attn), added score fields to outputs/CSV, removed eps/margin filtering in attn mode, updated README note.","created_at":"2026-01-16T16:43:14Z"},{"id":98,"issue_id":"nfp-lok","author":"TheDarkchip","text":"Ran small IOI-template attention evaluation: L5H5 mean attn(S2-\u003eS1+1)=0.113 (med 0.103), L6H9 mean=0.365 (med 0.362) over 12 examples. Discovery attn ranking on models/gpt2_small_ioi_L5.nfpt top20 includes L5H4 (~0.253) and L5H5 (~0.198).","created_at":"2026-01-16T16:44:43Z"},{"id":99,"issue_id":"nfp-lok","author":"TheDarkchip","text":"Review: build_prev_bigram currently uses last_seen of prev_tok that includes idx-1, so prev becomes q-1 for any q\u003e0 (active almost always true). That looks like attention-to-immediate-previous-token rather than previous occurrence of the q-1 token. If the intent is previous occurrence before q-1, need to track second-most or update last_seen one step later (see scripts/discover_gpt2_induction_targets.py around build_prev_bigram).","created_at":"2026-01-16T16:51:48Z"},{"id":100,"issue_id":"nfp-lok","author":"TheDarkchip","text":"Aligned bigram prev with literature: build_prev_bigram now uses prev-of-token at q-1 (i.e., previous occurrence before q-1) by shifting build_prev output; avoids always selecting q-1.","created_at":"2026-01-16T16:53:08Z"}]}
{"id":"nfp-lok.1","title":"State change: status → in_progress","description":"Set status to in_progress\n\nReason: Implementing attention-based discovery defaults","status":"closed","priority":4,"issue_type":"event","created_at":"2026-01-16T17:37:16.458576+01:00","created_by":"TheDarkchip","updated_at":"2026-01-16T18:10:17.114117+01:00","closed_at":"2026-01-16T18:10:17.114117+01:00","close_reason":"event marker","dependencies":[{"issue_id":"nfp-lok.1","depends_on_id":"nfp-lok","type":"parent-child","created_at":"2026-01-16T17:37:16.459255+01:00","created_by":"TheDarkchip"}]}
{"id":"nfp-lok.2","title":"Align bigram prev/active with canonical induction-head definition","description":"Review canonical definition: induction heads match the current token to a previous occurrence and copy the following token; in the framework paper this is described as searching for previous examples of the present token and copying the next token, implemented via key-shift / previous-token composition. Verify our prev/active construction matches this (vs using token at q-1). Update build_prev_bigram or add an explicit prev-mode (e.g., induction_prev = prev_occurrence(current)+1) and document the definition. Include a minimal unit-like check (small token example) in comments or a tiny test helper.","acceptance_criteria":"- prev/active definition and docs match the canonical induction-head algorithm; - discovery/scan/sweep use the canonical definition by default (or expose a clear mode switch); - add a small example or test helper showing expected prev mapping on a toy sequence.","status":"closed","priority":1,"issue_type":"task","owner":"robin.gieseke@me.com","created_at":"2026-01-16T18:00:29.450998+01:00","created_by":"TheDarkchip","updated_at":"2026-01-16T18:37:09.802599+01:00","closed_at":"2026-01-16T18:37:09.802599+01:00","close_reason":"done","dependencies":[{"issue_id":"nfp-lok.2","depends_on_id":"nfp-lok","type":"parent-child","created_at":"2026-01-16T18:00:29.454031+01:00","created_by":"TheDarkchip"}],"comments":[{"id":101,"issue_id":"nfp-lok.2","author":"TheDarkchip","text":"Updated build_prev_bigram to use prev_occurrence(token at q-1)+1 with example docstring; default prev-mode bigram now matches canonical induction definition.","created_at":"2026-01-16T17:36:58Z"}]}
{"id":"nfp-lok.3","title":"Add prefix-matching benchmark on repeated random sequences","description":"Canonical induction-head evaluations (Olsson et al., transformer-circuits) score heads on repeated random sequences (prefix matching) to isolate the mechanism. Add an optional mode to discovery/scan/sweep that generates repeated random token sequences (or uses provided synthetic prompts) and computes the attention-to-induction-target score on that distribution. This should avoid coupling to a specific nfpt prompt.","acceptance_criteria":"- new mode to generate repeated random sequences (configurable vocab/length/repeats); - report prefix-matching score in outputs/CSV/JSON; - docs mention how this aligns with the canonical measurement; - existing default path remains unchanged unless explicitly requested.","status":"closed","priority":2,"issue_type":"task","owner":"robin.gieseke@me.com","created_at":"2026-01-16T18:00:34.807508+01:00","created_by":"TheDarkchip","updated_at":"2026-01-16T21:19:35.392238+01:00","closed_at":"2026-01-16T21:19:35.392238+01:00","close_reason":"done","dependencies":[{"issue_id":"nfp-lok.3","depends_on_id":"nfp-lok","type":"parent-child","created_at":"2026-01-16T18:00:34.810238+01:00","created_by":"TheDarkchip"},{"issue_id":"nfp-lok.3","depends_on_id":"nfp-lok.2","type":"blocks","created_at":"2026-01-16T18:00:34.811446+01:00","created_by":"TheDarkchip"}],"comments":[{"id":107,"issue_id":"nfp-lok.3","author":"TheDarkchip","text":"Added --synthetic mode to scan_gpt2_induction_sound.py (generate repeated-random pattern prompts via generate_rigorous_induction.py) with metadata in reports; documented prefix-matching benchmark in README and sweep docstring.","created_at":"2026-01-16T20:19:31Z"}]}
{"id":"nfp-lok.4","title":"Add copying/OV score to attention-based ranking","description":"Induction heads are characterized by both prefix matching (attention) and copying via OV to the next-token logits. Add an optional copying/OV score (e.g., logit contribution of the attended token's successor or OV-projection alignment) and allow ranking/filtering by attention+copying. This keeps attention-only discovery aligned with literature's full induction head definition.","acceptance_criteria":"- compute a copying/OV score per head (clearly defined in docs); - expose score in outputs/CSV/JSON; - allow ranking/filtering by copying score or combined score; - defaults preserve current behavior.","status":"closed","priority":2,"issue_type":"task","owner":"robin.gieseke@me.com","created_at":"2026-01-16T18:00:39.4892+01:00","created_by":"TheDarkchip","updated_at":"2026-01-16T19:02:50.312331+01:00","closed_at":"2026-01-16T19:02:50.312331+01:00","close_reason":"done","dependencies":[{"issue_id":"nfp-lok.4","depends_on_id":"nfp-lok","type":"parent-child","created_at":"2026-01-16T18:00:39.492325+01:00","created_by":"TheDarkchip"},{"issue_id":"nfp-lok.4","depends_on_id":"nfp-lok.2","type":"blocks","created_at":"2026-01-16T18:00:39.493647+01:00","created_by":"TheDarkchip"}],"comments":[{"id":105,"issue_id":"nfp-lok.4","author":"TheDarkchip","text":"Added copy/OV scoring and attn_copy score modes to discovery/scan/sweep, with min-copy filtering and JSON/CSV outputs for copy metrics.","created_at":"2026-01-16T18:02:36Z"}]}
{"id":"nfp-lok.5","title":"Support multi-lag induction target definitions","description":"Transformer-circuits notes that some induction heads attend further than one token back (e.g., keys composed with earlier-token heads). Add a configurable lag (or k-gram) option so discovery can target induction patterns beyond q-1 (e.g., lag=2 uses token at q-2 to define the induction target).","acceptance_criteria":"- add --prev-lag or --k to define induction target beyond q-1; - update discovery/scan/sweep to forward the option; - document how this relates to multi-step induction in the literature.","status":"open","priority":3,"issue_type":"task","owner":"robin.gieseke@me.com","created_at":"2026-01-16T18:00:43.941794+01:00","created_by":"TheDarkchip","updated_at":"2026-01-16T18:00:43.941794+01:00","dependencies":[{"issue_id":"nfp-lok.5","depends_on_id":"nfp-lok","type":"parent-child","created_at":"2026-01-16T18:00:43.944725+01:00","created_by":"TheDarkchip"},{"issue_id":"nfp-lok.5","depends_on_id":"nfp-lok.2","type":"blocks","created_at":"2026-01-16T18:00:43.946+01:00","created_by":"TheDarkchip"}]}
{"id":"nfp-lok.6","title":"Add canonical bigram prev/active definitions in InductionPrompt","description":"Literature defines induction heads as matching the **current token** to a previous occurrence and copying the token that followed that occurrence (A B ... A -\u003e B). In terms of attention targets, this is the **shifted** previous occurrence: prev_induction(q) = prev_occurrence(tokens q) + 1. Current prevOfTokens/activeOfTokens match the current token without the +1 shift. Add canonical shifted-prev definitions (e.g., prevOfTokensShift, activeOfTokensShift) and analogous period-based versions, with docstrings and toy example. Include lemmas mirroring prevOfTokens_spec/mem_activeOfTokens.","acceptance_criteria":"- new prev/active definitions capture shifted-prev induction target; - lemmas specify behavior and maximality; - docstring cites canonical induction head algorithm and includes toy sequence mapping.","status":"closed","priority":1,"issue_type":"task","owner":"robin.gieseke@me.com","created_at":"2026-01-16T18:05:28.015929+01:00","created_by":"TheDarkchip","updated_at":"2026-01-16T18:45:28.55374+01:00","closed_at":"2026-01-16T18:45:28.55374+01:00","close_reason":"done","dependencies":[{"issue_id":"nfp-lok.6","depends_on_id":"nfp-lok","type":"parent-child","created_at":"2026-01-16T18:05:28.01855+01:00","created_by":"TheDarkchip"}],"comments":[{"id":102,"issue_id":"nfp-lok.6","author":"TheDarkchip","text":"Added shifted prev/active definitions (tokens + period) with docstring example and mem/spec lemmas in InductionPrompt.","created_at":"2026-01-16T17:45:20Z"}]}
{"id":"nfp-lok.7","title":"Add canonical induction prev-mode to GPT-2 head input builder","description":"Sound.Gpt2.buildInductionHeadInputs currently uses period-based prev/active that match the current token. Add a canonical bigram option (or a new buildInductionHeadInputsBigram) that uses the new prevOfPrevTokens/activeOfPrevTokens (or period-shifted analog) so the default Lean-side builder can align with the literature definition when desired. Update docstrings to clarify which definition is in use.","acceptance_criteria":"- builder exposes a bigram/canonical mode; - docstrings state which induction definition is used; - any downstream uses are updated or explicitly opt in/out.","status":"closed","priority":1,"issue_type":"task","owner":"robin.gieseke@me.com","created_at":"2026-01-16T18:05:47.546322+01:00","created_by":"TheDarkchip","updated_at":"2026-01-16T18:46:39.644642+01:00","closed_at":"2026-01-16T18:46:39.644642+01:00","close_reason":"done","dependencies":[{"issue_id":"nfp-lok.7","depends_on_id":"nfp-lok","type":"parent-child","created_at":"2026-01-16T18:05:47.549418+01:00","created_by":"TheDarkchip"},{"issue_id":"nfp-lok.7","depends_on_id":"nfp-lok.6","type":"blocks","created_at":"2026-01-16T18:05:47.550743+01:00","created_by":"TheDarkchip"}],"comments":[{"id":103,"issue_id":"nfp-lok.7","author":"TheDarkchip","text":"Added buildInductionHeadInputsShift using activeOfPeriodShift/prevOfPeriodShift; docstrings clarify unshifted vs shifted builders.","created_at":"2026-01-16T17:46:36Z"}]}
{"id":"nfp-lok.8","title":"Add induction prev-spec invariants and proofs for builders","description":"The literature assumes induction attention targets are strictly in the past (prev q \u003c q) and drawn from earlier occurrences. Add a small predicate/structure (e.g., InductionPrevSpec inputs) expressing canonical constraints (prev q \u003c q for active q, prev derived from token/bigram rule). Prove that the new bigram prompt builders satisfy it, and consider exposing it in certificate checks as optional assumptions/docstrings.","acceptance_criteria":"- predicate for prev/active invariants is defined; - builder proofs show the canonical prompt meets it; - docs clarify that induction bounds assume causal prev targets.","status":"closed","priority":2,"issue_type":"task","owner":"robin.gieseke@me.com","created_at":"2026-01-16T18:05:54.575916+01:00","created_by":"TheDarkchip","updated_at":"2026-01-16T18:56:43.475447+01:00","closed_at":"2026-01-16T18:56:43.475447+01:00","close_reason":"done","dependencies":[{"issue_id":"nfp-lok.8","depends_on_id":"nfp-lok","type":"parent-child","created_at":"2026-01-16T18:05:54.578867+01:00","created_by":"TheDarkchip"},{"issue_id":"nfp-lok.8","depends_on_id":"nfp-lok.6","type":"blocks","created_at":"2026-01-16T18:05:54.580341+01:00","created_by":"TheDarkchip"}],"comments":[{"id":104,"issue_id":"nfp-lok.8","author":"TheDarkchip","text":"Added InductionPrevSpecPeriodShift/InductionPrevSpecTokensShift and InductionPrevInPast predicates; proved shifted GPT-2 builder satisfies period spec and documented prev-in-past assumption for period\u003e1.","created_at":"2026-01-16T17:56:40Z"}]}
{"id":"nfp-m4b","title":"Remove obsolete scripts and refresh docs","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-17T10:30:55.825187+01:00","updated_at":"2026-01-17T10:30:55.892841+01:00","closed_at":"2026-01-17T10:30:55.892841+01:00","close_reason":"Removed variance_lower_bound_check.py and updated docs to clarify untrusted direction search."}
{"id":"nfp-m5i","title":"Add exact induction diagnostic (seq_len=50)","description":"Implement script that reproduces literature diagnostic: batch of repeated random token sequences (seq_len=50, pattern_len=25), compute induction stripe attention and prev-token attention from GPT-2 small output attentions, rank heads.","notes":"Added scripts/diagnose_induction_heads.py with seq_len=50 repeated-random diagnostic using GPT-2 attentions (attn_implementation=eager). Ran sample batch=30: L5H5 appears in stripe top10; prev-token head L5H12 top.","status":"closed","priority":2,"issue_type":"task","assignee":"TheDarkchip","owner":"robin.gieseke@me.com","created_at":"2026-01-16T22:44:32.496981+01:00","created_by":"TheDarkchip","updated_at":"2026-01-16T22:46:28.913021+01:00","closed_at":"2026-01-16T22:46:28.913023+01:00"}
{"id":"nfp-mt4","title":"Move residual/downstream cert construction out of trusted Bounds","description":"Summary\n- Trusted `Nfp.Sound.Bounds.*` builds residual/downstream certificates directly from matrices/weights.\n\nViolation\n- The trusted side should only check explicit certificates, not compute them from raw matrices or model weights.\n\nEvidence\n- `Nfp/Sound/Bounds/MatrixNorm/Basic.lean`:\n  - `buildResidualIntervalCertFromMatrix`, `buildDownstreamLinearCert` construct certs from `W` and bounds.\n- `Nfp/Sound/Bounds/Transformer/Basic.lean`:\n  - `gpt2ResidualIntervalBoundsActive_sound` packages computed bounds into a `ResidualIntervalCert`.\n\nImpact\n- Certificate generation happens inside the trusted boundary, defeating the small-checker split.\n\nProposed Fix\n- Move matrix/transformer bound computation to untrusted tooling; produce explicit `ResidualIntervalCert` / `DownstreamLinearCert` artifacts.\n- Keep only `checkResidualIntervalCert` / `checkDownstreamLinearCert` (and soundness theorems) in trusted code.\n- Ensure CLI or IO paths consume explicit certs rather than raw matrices.\n\nDependencies / Blockers\n- None, but may need to define external cert serialization formats if not already present.\n","notes":"Removed residual cert packaging in Bounds/Transformer (deleted gpt2ResidualIntervalBoundsActive_sound) and in Sound/Induction/EndToEnd; no ResidualInterval/Downstream cert construction remains in trusted Bounds.","status":"closed","priority":2,"issue_type":"task","owner":"robin.gieseke@me.com","created_at":"2026-01-17T07:00:40.564882+01:00","created_by":"TheDarkchip","updated_at":"2026-01-17T08:10:41.077751+01:00","closed_at":"2026-01-17T08:10:41.077754+01:00"}
{"id":"nfp-mx7","title":"Formalize induction circuit (prev-token head + induction head)","description":"Literature defines induction heads as part of a 2-head circuit: a previous-token head writes token t_k into residual at k+1, and the induction head attends to the previous occurrence of the current token and copies the next token. Add a sound circuit-level certificate path that composes a prev-token head (earlier layer) with an induction head (later layer), including formal definitions and verification over repeated-pattern prompts. Wire a CLI entry (or extend existing induction certify) to certify the composed circuit.","acceptance_criteria":"- Lean definitions for a prev-token head + induction head circuit with explicit prev/active maps.\\n- Sound theorem: circuit output matches (or lower-bounds) next-token copy on active positions under documented assumptions.\\n- CLI path (or script) to certify the circuit on model binaries with period prompts.\\n- docs updated to clarify circuit-level canonical definition vs single-head approximation.","status":"open","priority":2,"issue_type":"task","owner":"robin.gieseke@me.com","created_at":"2026-01-17T02:35:48.168726+01:00","created_by":"TheDarkchip","updated_at":"2026-01-17T02:35:48.168726+01:00","labels":["mode:in_progress"],"dependencies":[{"issue_id":"nfp-mx7","depends_on_id":"nfp-8or","type":"blocks","created_at":"2026-01-17T02:35:48.170174+01:00","created_by":"TheDarkchip"},{"issue_id":"nfp-mx7","depends_on_id":"nfp-ywe","type":"blocks","created_at":"2026-01-17T02:35:48.171205+01:00","created_by":"TheDarkchip"}],"comments":[{"id":114,"issue_id":"nfp-mx7","author":"TheDarkchip","text":"Added shift-prev capability in trusted IO/CLI and discovery scripts (period_shift). This aligns with literature describing induction heads as copying the next token after a previous occurrence (prev-token head + induction head circuit). Next step: formalize the circuit-level composition and cert path.","created_at":"2026-01-17T01:42:29Z"},{"id":117,"issue_id":"nfp-mx7","author":"TheDarkchip","text":"Documented prev-shift (shifted prev/active) in docs/induction_cert_audit.md as a head-level approximation of the canonical prev-token+induction circuit.","created_at":"2026-01-17T01:49:35Z"},{"id":118,"issue_id":"nfp-mx7","author":"TheDarkchip","text":"Added Nfp/Model/InductionCircuit.lean with circuit-level shifted-prev specs and a diagnostic-period lifting lemma (requires 0\u003cperiod). Imported into Nfp/Model.lean.","created_at":"2026-01-17T01:51:45Z"},{"id":119,"issue_id":"nfp-mx7","author":"TheDarkchip","text":"Added circuit-layer spec: Nfp/Circuit/Layers/Induction/Circuit.lean defines InductionCircuitSpec (prev-token head + induction head) plus PrevTokenSpec alias in Basic.lean; aggregated via Nfp/Circuit/Layers/Induction.lean. Build passes.","created_at":"2026-01-17T05:28:08Z"},{"id":120,"issue_id":"nfp-mx7","author":"TheDarkchip","text":"Added IO plumbing for certify_circuit_model (prev-token head + induction head) in Nfp/IO/InductionHead/Circuit.lean and CLI command wiring in Nfp/Cli.lean. Circuit spec definitions are now in Nfp/Circuit/Layers/Induction/Circuit.lean.","created_at":"2026-01-17T05:33:57Z"},{"id":121,"issue_id":"nfp-mx7","author":"TheDarkchip","text":"Added InductionCircuitSpec_compose lemma (circuit composition) plus InductionSpec_def/PrevTokenSpec_def in Induction.Basic; avoids flexible simp warnings. Build passes.","created_at":"2026-01-17T05:39:17Z"},{"id":122,"issue_id":"nfp-mx7","author":"TheDarkchip","text":"Updated certify_circuit_model: induction head now uses shifted prev (canonical circuit); added positive-period guard and updated CLI/doc messaging.","created_at":"2026-01-17T05:46:17Z"}]}
{"id":"nfp-mx7.1","title":"State change: mode → in_progress","description":"Set mode to in_progress\n\nReason: Started circuit-level spec scaffolding (shifted prev, docs, new model spec).","status":"open","priority":4,"issue_type":"event","created_at":"2026-01-17T02:52:43.585164+01:00","created_by":"TheDarkchip","updated_at":"2026-01-17T02:52:43.585164+01:00","dependencies":[{"issue_id":"nfp-mx7.1","depends_on_id":"nfp-mx7","type":"parent-child","created_at":"2026-01-17T02:52:43.585797+01:00","created_by":"TheDarkchip"}]}
{"id":"nfp-mzt","title":"Bridge head-level induction certificates to block/model logits","description":"Add formal bridge theorems from head-level certificates (attention + value + logit-diff bounds) to block-level or end-to-end model logit bounds.","status":"open","priority":2,"issue_type":"task","owner":"robin.gieseke@me.com","created_at":"2026-01-13T08:09:33.475601+01:00","created_by":"TheDarkchip","updated_at":"2026-01-13T08:09:33.475601+01:00","dependencies":[{"issue_id":"nfp-mzt","depends_on_id":"nfp-axz","type":"relates-to","created_at":"2026-01-14T01:54:44.801985+01:00","created_by":"TheDarkchip"},{"issue_id":"nfp-mzt","depends_on_id":"nfp-4z9","type":"relates-to","created_at":"2026-01-14T01:54:59.551515+01:00","created_by":"TheDarkchip"}],"comments":[{"id":7,"issue_id":"nfp-mzt","author":"TheDarkchip","text":"Added bridge lemmas in LogitDiff.lean: headLogitDiff_eq_direction_dot_headOutput and logitDiffLowerBound_with_residual (compose head logit-diff bound with residual interval bounds). Updated CLAIMS.md and docs/induction_cert_audit.md to reflect the partial bridge; full end-to-end model semantics still open.","created_at":"2026-01-13T11:18:45Z"},{"id":8,"issue_id":"nfp-mzt","author":"TheDarkchip","text":"Added gpt2ResidualIntervalBoundsActive_sound in Sound/Bounds/Transformer.lean to package GPT-2 residual bounds into a ResidualIntervalCert with ResidualIntervalBounds + output_mem for transformerStackFinalReal (requires hne/heps/hsqrt). Updated CLAIMS.md and docs/induction_cert_audit.md accordingly. Next gap: connect head-level logit-diff contribution through the stack (residual is not just additive after LN/MLP).","created_at":"2026-01-13T11:31:05Z"},{"id":9,"issue_id":"nfp-mzt","author":"TheDarkchip","text":"Added logitDiffLowerBound_with_output_intervals in Sound/Induction/LogitDiff.lean: combines head logit-diff LB with head-output intervals and downstream output intervals via interval subtraction (residual = output - headOutput). Updated CLAIMS.md, docs/induction_cert_audit.md, SOUNDNESS_LIMITATIONS.md to reflect this partial bridge.","created_at":"2026-01-13T11:42:26Z"},{"id":10,"issue_id":"nfp-mzt","author":"TheDarkchip","text":"Added EndToEnd.lean with logitDiffLowerBound_end_to_end_gpt2: instantiates logitDiffLowerBound_with_output_intervals using HeadOutputIntervalSound and gpt2ResidualIntervalBoundsActive_sound to bound direction·transformerStackFinalReal. Updated Induction aggregator and MODULE_MAP. Build clean.","created_at":"2026-01-13T11:49:54Z"},{"id":11,"issue_id":"nfp-mzt","author":"TheDarkchip","text":"Wired nfp induction certify_end_to_end_model to optionally derive head-output interval bounds when --layer/--head are provided (with optional --period). CLI now computes interval-composed bound via head output intervals and residual intervals, reports intervalLB/effectiveLB, and keeps base downstream bound as fallback.","created_at":"2026-01-13T12:21:53Z"},{"id":27,"issue_id":"nfp-mzt","author":"TheDarkchip","text":"SOUNDNESS_LIMITATIONS note: end-to-end bridge still ignores LN/MLP propagation of head outputs and assumes unembedding columns define logits. Consider extending the bridge to model block semantics (residual/LN/MLP) and making the logit map explicit in the proof chain.","created_at":"2026-01-14T00:52:28Z"}]}
{"id":"nfp-n7u","title":"Add bias-aware attention projections in circuit-layer attention","description":"Circuit-layer attention wiring (Nfp/Circuit/Layers/Attention.lean) uses batchedLinearTyped (no bias) for Q/K/V and output projections. GPT-2 uses affine projections with biases. Add bias-aware variants (batchedAffineTyped) and update any GPT-style circuit constructors so circuit semantics match literature/model definitions when used.","acceptance_criteria":"- new bias-aware Q/K/V/out projection combinators exist; - GPT-style circuit constructors can choose biased versions; - docs clarify bias handling and defaults.","status":"open","priority":3,"issue_type":"task","owner":"robin.gieseke@me.com","created_at":"2026-01-16T18:25:39.742296+01:00","created_by":"TheDarkchip","updated_at":"2026-01-16T18:25:39.742296+01:00"}
{"id":"nfp-o21","title":"Prune obsolete artifacts and refresh docs","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-17T10:34:35.897989+01:00","updated_at":"2026-01-17T10:34:35.976308+01:00","closed_at":"2026-01-17T10:34:35.976308+01:00","close_reason":"Removed obsolete script and updated docs for untrusted direction search."}
{"id":"nfp-o7c","title":"Document literature-aligned induction diagnostics","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-17T10:12:29.597819+01:00","updated_at":"2026-01-17T10:12:29.661694+01:00","closed_at":"2026-01-17T10:12:29.661694+01:00","close_reason":"Added literature alignment notes to diagnostics and cert generator docs."}
{"id":"nfp-osv","title":"Expose split-budget knobs for induction head certification","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-13T00:52:32.742247+01:00","updated_at":"2026-01-13T01:11:40.876901+01:00","closed_at":"2026-01-13T01:11:40.876901+01:00","close_reason":"Closed"}
{"id":"nfp-p98","title":"General induction head certification pipeline (any head)","description":"Build a general, scriptable pipeline that certifies any specified head from a model file. Provide a single entry point (CLI or script) that: (1) ensures a diagnostic prompt (periodic tokens) is present or creates a model with such tokens, (2) runs nfp induction certify_head_model_auto (or nonvacuous) for a chosen layer/head, and (3) writes/logs the resulting certificate outputs. Include usage docs and make it compatible with uv run.","notes":"Added --skip-logit-diff flag (streamlined + advanced) to bypass expensive logit-diff check; updated certify_induction_head.py to support presets/advanced fallback. Verified working run on models/gpt2_rigorous_seq32.nfpt L4H1 period 20 with --preset fast --min-margin \u003cnegative\u003e --max-eps 1 --skip-logit-diff (prints ok).","status":"closed","priority":2,"issue_type":"task","owner":"robin.gieseke@me.com","created_at":"2026-01-16T23:29:42.844212+01:00","created_by":"TheDarkchip","updated_at":"2026-01-17T00:26:23.973806+01:00","closed_at":"2026-01-17T00:26:23.973808+01:00"}
{"id":"nfp-pgc","title":"Merge refactor/module-structure-nfp-e53 into audit-induction-cert","notes":"Merged refactor/module-structure-nfp-e53 into audit-induction-cert.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-14T01:47:18.190665+01:00","updated_at":"2026-01-14T01:49:02.950839+01:00","closed_at":"2026-01-14T01:49:02.950845+01:00"}
{"id":"nfp-rd4","title":"Make cert generator layer/head indexing 1-based","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-17T10:07:28.552348+01:00","updated_at":"2026-01-17T10:07:28.620159+01:00","closed_at":"2026-01-17T10:07:28.620159+01:00","close_reason":"Generator now accepts 1-based layer/head and converts internally; docs updated."}
{"id":"nfp-sd6","title":"Refresh docs/claims for Python generation + Lean verification split","description":"README/CLAIMS/SOUNDNESS_LIMITATIONS/induction_cert_audit still mention Lean-side cert builders and CLI commands that no longer exist. Update docs to describe Python scripts generating certificates and Lean CLI verifying explicit certs only.","acceptance_criteria":"- README reflects current CLI (induction certify/certify_nonvacuous/head_cert_check)\\n- CLAIMS updated to remove Lean builder claims; only checker claims remain\\n- SOUNDNESS_LIMITATIONS + docs/induction_cert_audit updated to reflect explicit cert verification\\n- No mentions of removed commands (certify_head*, certify_sound, end_to_end, etc.)","status":"closed","priority":2,"issue_type":"task","owner":"robin.gieseke@me.com","created_at":"2026-01-17T08:43:56.605475+01:00","created_by":"TheDarkchip","updated_at":"2026-01-17T09:02:00.725645+01:00","closed_at":"2026-01-17T09:02:00.725645+01:00","close_reason":"Closed","labels":["docs","split"]}
{"id":"nfp-skw","title":"Move linear helper folds out of trusted Sound","description":"Nfp.Sound.Linear.FinFold provides generic fold/sum helpers; this is not certificate checking and should be untrusted.","notes":"Moved Nfp.Sound.Linear.FinFold to Nfp.Linear.FinFold and removed Sound reexport.","status":"closed","priority":2,"issue_type":"task","owner":"robin.gieseke@me.com","created_at":"2026-01-17T08:34:47.529342+01:00","created_by":"TheDarkchip","updated_at":"2026-01-17T08:34:51.716619+01:00","closed_at":"2026-01-17T08:34:51.716621+01:00"}
{"id":"nfp-snh","title":"Refactor MatrixNorm interval bounds proofs","description":"Refactor Nfp/Sound/Bounds/MatrixNorm/Interval.lean to reduce proof churn and simplify interval bound lemmas.","status":"closed","priority":2,"issue_type":"task","owner":"robin.gieseke@me.com","created_at":"2026-01-12T15:54:50.752468+01:00","created_by":"TheDarkchip","updated_at":"2026-01-12T16:01:12.070535+01:00","closed_at":"2026-01-12T16:01:12.070535+01:00","close_reason":"Closed","comments":[{"id":1,"issue_id":"nfp-snh","author":"TheDarkchip","text":"Refactored foldl_pair helper for pair foldl projections in MatrixNorm interval bounds; adjusted proofs to use Prod.fst/Prod.snd. Fixed CoreSound cdot lint warnings encountered during build.","created_at":"2026-01-12T15:01:08Z"}]}
{"id":"nfp-suu","title":"Include attention output bias in head certification","description":"Current head certification ignores the shared attention output bias; this is an extra residual-stream write alongside OV and can shift logit-diff bounds. Add it to head output/value bounds so QK/OV path decomposition matches model semantics.","acceptance_criteria":"Head-output/value bounds account for attn output bias in certify_head/certify_head_model; docs updated; lake build --wfail passes.","status":"open","priority":2,"issue_type":"task","owner":"robin.gieseke@me.com","created_at":"2026-01-14T01:51:35.915011+01:00","created_by":"TheDarkchip","updated_at":"2026-01-14T01:51:35.915011+01:00","dependencies":[{"issue_id":"nfp-suu","depends_on_id":"nfp-axz","type":"relates-to","created_at":"2026-01-14T01:54:55.041176+01:00","created_by":"TheDarkchip"}]}
{"id":"nfp-w1y","title":"Find nonvacuous induction head certificate","description":"Goal: obtain a nonvacuous induction head certificate (positive logitDiffLB). Plan: generate strong induction model (scripts/generate_induction_data.py), run discovery with logit mode to choose head + direction, then run nfp induction certify_nonvacuous (model) for that head. Track any blockers (direction, eps/margin constraints, runtime).","notes":"Tried nonvacuous certify on gpt2_rigorous_seq32 with period=31 (active=1), min-active=1, max-eps=1; logit-diff still extremely slow (timed out at 5m) even for single query. Q-only early-exit shows epsAt=1. Tried L1H6 (1-based) with/without period and preset fast; eps still 1 \u003e 1/2; nonvacuous fails. Preset tight with skip-logit-diff timed out at 120s.","status":"closed","priority":1,"issue_type":"task","owner":"robin.gieseke@me.com","created_at":"2026-01-17T01:09:32.990461+01:00","created_by":"TheDarkchip","updated_at":"2026-01-17T09:53:35.883324+01:00","closed_at":"2026-01-17T09:53:35.883324+01:00","close_reason":"Found default-gated induction head cert (layer0 head5, random seed 0) and verified via CLI.","comments":[{"id":108,"issue_id":"nfp-w1y","author":"TheDarkchip","text":"Tried gpt2_rigorous_seq32.nfpt with correct period=16. discover_gpt2_induction_targets (score-mode=stripe, prev-mode=period) ranks L1H6 top (stripeMean≈0.636) and L5H5 ≈0.121. Certify attempts: L1H6 with max-eps 1/2 fails (epsAt=1). Nonvacuous run with max-eps 1 times out; q-only debug shows epsAt=1. L5H5 fails min-margin 0 (margin hugely negative). Formal epsAt bound still stuck at 1 despite untrusted eps ~0.446.","created_at":"2026-01-17T00:49:00Z"},{"id":113,"issue_id":"nfp-w1y","author":"TheDarkchip","text":"Tried shifted prev/active (--prev-shift) on gpt2_rigorous_seq32 L0H5 (period=16): q-only debug still epsAt=1 (prev=16).","created_at":"2026-01-17T01:29:21Z"}]}
{"id":"nfp-w4w","title":"Interpret circuit_copy rankings vs canonical induction heads","description":"Compare circuit_copy top pairs against canonical induction heads in literature (e.g., GPT-2 small L8H3/L8H2) and document alignment or gaps.","notes":"Seq_len=50 diagnostic averaged over 5 seeds (1001-1005): stripe top heads L1H6, L4H1, L1H2, L2H12; L5H5 ranks 8 by stripe; prev-token heads L5H12 top, then L4H8, L3H3. Reports: reports/induction_diagnostic_seq50_seed*.txt","status":"open","priority":2,"issue_type":"task","assignee":"TheDarkchip","owner":"robin.gieseke@me.com","created_at":"2026-01-16T22:26:25.476401+01:00","created_by":"TheDarkchip","updated_at":"2026-01-16T22:50:59.857227+01:00"}
{"id":"nfp-w8s","title":"Align Lean CLI indexing with literature (1-based)","description":"Align CLI interface with literature by accepting 1-based layer/head indices by default (as used in scripts). Internally convert to zero-based. Keep an explicit compatibility flag or suffix (e.g., --zero-based) for legacy usage and update docs/examples accordingly.","acceptance_criteria":"- CLI accepts 1-based layer/head indices; - internal conversion is explicit and tested; - docs/README and help text clearly state indexing; - optional flag supports legacy zero-based calls.","status":"closed","priority":2,"issue_type":"task","owner":"robin.gieseke@me.com","created_at":"2026-01-16T17:14:28.62198+01:00","created_by":"TheDarkchip","updated_at":"2026-01-16T19:10:08.215997+01:00","closed_at":"2026-01-16T19:10:08.215997+01:00","close_reason":"done","comments":[{"id":106,"issue_id":"nfp-w8s","author":"TheDarkchip","text":"Updated CLI to treat layer/head as 1-based by default with --zero-based legacy flag; added conversions in handlers and updated README/help text.","created_at":"2026-01-16T18:09:55Z"}]}
{"id":"nfp-wyz","title":"Remove/update Python scripts that call deprecated Lean generation commands","description":"Several scripts (certify_induction_head.py, scan_gpt2_induction_sound.py, sweep_gpt2_induction_nonvacuous.py, discover_gpt2_induction_targets.py, build_gpt2_head_inputs.py) reference removed CLI commands (certify_head_model*, certify_head, certify_end_to_end). They violate the Python-only generation split and are now broken. Remove or refactor them to pure Python generation + nfp induction certify check.","acceptance_criteria":"- Scripts that call deprecated CLI commands are removed or updated to use explicit cert generation + nfp induction certify\\n- Docs no longer reference removed scripts/commands","status":"closed","priority":2,"issue_type":"task","owner":"robin.gieseke@me.com","created_at":"2026-01-17T08:50:41.108215+01:00","created_by":"TheDarkchip","updated_at":"2026-01-17T09:02:00.72851+01:00","closed_at":"2026-01-17T09:02:00.72851+01:00","close_reason":"Closed","labels":["cleanup","python","split"]}
{"id":"nfp-x8g","title":"Streamline CLI options and induction certification UX","description":"Reduce option surface area, add presets/shortcuts, and improve defaults for induction certification commands without losing advanced configurability.","status":"closed","priority":2,"issue_type":"task","owner":"robin.gieseke@me.com","created_at":"2026-01-13T07:53:31.417957+01:00","created_by":"TheDarkchip","updated_at":"2026-01-13T08:00:52.123717+01:00","closed_at":"2026-01-13T08:00:52.123717+01:00","close_reason":"Closed"}
{"id":"nfp-xh8","title":"Fix stack overflow when certifying GPT-2 induction certs","description":"Stack overflow when running induction certification on seq=256 GPT-2 certs. Repro: generate softmax/value certs via scripts/build_gpt2_induction_cert_from_binary.py on /Users/robin/Uni/iml/gpt2_rigorous.nfpt (layer 5 head 1, target 2835, negative 4236). Then run lake exe nfp induction advanced certify with those files; crash with stack overflow. End-to-end model path also crashes even with dummy residual interval. Suspect checkSoftmaxMarginCert / finset recursion; consider loop/array-based checks.","status":"closed","priority":2,"issue_type":"task","owner":"robin.gieseke@me.com","created_at":"2026-01-13T16:19:57.127206+01:00","created_by":"TheDarkchip","updated_at":"2026-01-13T16:59:56.811923+01:00","closed_at":"2026-01-13T16:59:56.811923+01:00","close_reason":"Stack overflow resolved by array-backed parse state; GPT-2 cert check runs.","labels":["bug"],"comments":[{"id":12,"issue_id":"nfp-xh8","author":"TheDarkchip","text":"Switched SoftmaxMargin ParseState matrices from nested function updates to Array-of-Array storage and use Array.all in finalize; lookup now uses array indexing. Stack overflow gone: ok: softmax-margin certificate accepted (seq=256, active=2) succeeds. With values: fails on negative logitDiffLB (no crash).","created_at":"2026-01-13T15:59:38Z"}]}
{"id":"nfp-ywe","title":"Add circuit-level induction discovery (prev-token head + induction head)","description":"Literature defines induction as a circuit: previous-token head writes previous token into residual, induction head attends to it and copies the continuation. Add an optional discovery mode that composes a candidate previous-token head (earlier layer) with a candidate induction head (later layer) and scores prefix-matching on the composed circuit.\\n\\nAcceptance criteria:\\n- discovery mode that evaluates (prev-head, induction-head) pairs on synthetic prompts.\\n- clear scoring definition and outputs (CSV/JSON).\\n- docs note circuit-level canonical definition.\\n\\nDepends on:\\n- nfp-140 (real activation-based discovery).","status":"open","priority":3,"issue_type":"task","owner":"robin.gieseke@me.com","created_at":"2026-01-16T21:32:10.496585+01:00","created_by":"TheDarkchip","updated_at":"2026-01-16T21:32:10.496585+01:00","comments":[{"id":115,"issue_id":"nfp-ywe","author":"TheDarkchip","text":"Added  prev-mode in discovery scripts and  in certified model input path to align with canonical induction circuit (prev-token head + induction head). Next step is to formalize the circuit composition and add a cert path (see nfp-mx7).","created_at":"2026-01-17T01:43:22Z"},{"id":116,"issue_id":"nfp-ywe","author":"TheDarkchip","text":"Added prev-shift and period_shift support to align discovery/cert with canonical induction circuit (prev-token head + induction head). Next step is to formalize circuit composition and add cert path (see nfp-mx7).","created_at":"2026-01-17T01:43:27Z"}]}
{"id":"nfp-z1j","title":"Audit induction-head certification soundness claim","description":"Review proof chain for induction-head certification and document scope/limitations with mechinterp framing.","status":"closed","priority":2,"issue_type":"task","owner":"robin.gieseke@me.com","created_at":"2026-01-13T08:09:10.323741+01:00","created_by":"TheDarkchip","updated_at":"2026-01-13T08:10:29.361118+01:00","closed_at":"2026-01-13T08:10:29.361118+01:00","close_reason":"Closed"}
